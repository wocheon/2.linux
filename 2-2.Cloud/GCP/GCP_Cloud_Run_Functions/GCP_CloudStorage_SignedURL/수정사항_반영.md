
## 수정사항 
- 기존 버킷 디렉토리 구조 변경
    - `버킷/pdf_export/국가코드/일자/주식코드/파일`
- https 연결로 변경 
    - LB 내 HTTPS(443) 프론트엔드 추가
    - gcp 인증서를 추가하여 LB에 연결

> init.sql
```sql
CREATE TABLE gcs_bucket_test.pdf_bucket_file_table (
  crawl_data_id INT AUTO_INCREMENT PRIMARY KEY,
  stock_exch_contry_code VARCHAR(10),
  crawled_date VARCHAR(50),
  stock_issue_code VARCHAR(20),
  file_name VARCHAR(100)
) CHARACTER SET = utf8mb4 COLLATE = utf8mb4_unicode_ci;
```

> insert_test_data.py
```py
import os
import pymysql
from google.cloud import storage

def get_db_connection():
    try:
        conn = pymysql.connect(
            host=os.environ.get('DB_HOST'),
            port=int(os.environ.get('DB_PORT', 3306)),
            user=os.environ.get('DB_USER'),
            password=os.environ.get('DB_PASSWORD'),
            db=os.environ.get('DB_NAME'),
            charset='utf8mb4',
            cursorclass=pymysql.cursors.DictCursor,
            autocommit=False
        )
        print("DEBUG: DB connection established")
        return conn
    except Exception as e:
        print(f"ERROR: DB connection failed - {str(e)}")
        raise

# 버킷 디렉토리 구조 변경 - 버킷/pdf_export/국가코드/일자/주식코드/파일

def list_and_store_files(bucket_name, root_prefix="pdf_export/"):
    try:
        client = storage.Client()
        print(f"DEBUG: Storage client created for bucket '{bucket_name}'")
        bucket = client.bucket(bucket_name)
        connection = get_db_connection()

        try:
            with connection.cursor() as cursor:
                # 1단계: 국가코드 목록
                root_blobs_iter = bucket.list_blobs(prefix=root_prefix, delimiter='/')
                for root_page in root_blobs_iter.pages:
                    country_codes = root_page.prefixes
                    print(f"DEBUG: Found country codes: {country_codes}")

                    for country_prefix in country_codes:
                        country_code = country_prefix[len(root_prefix):].rstrip('/')

                        # 2단계: 일자 목록
                        date_blobs_iter = bucket.list_blobs(prefix=country_prefix, delimiter='/')
                        for date_page in date_blobs_iter.pages:
                            date_codes = date_page.prefixes
                            print(f"DEBUG: Found date codes under {country_code}: {date_codes}")

                            for date_prefix in date_codes:
                                date_code = date_prefix[len(country_prefix):].rstrip('/')

                                # 3단계: 주식코드 목록
                                stock_blobs_iter = bucket.list_blobs(prefix=date_prefix, delimiter='/')
                                for stock_page in stock_blobs_iter.pages:
                                    stock_codes = stock_page.prefixes
                                    print(f"DEBUG: Found stock codes under {country_code}/{date_code}: {stock_codes}")

                                    for stock_prefix in stock_codes:
                                        stock_code = stock_prefix[len(date_prefix):].rstrip('/')

                                        # 4단계: 주식코드 내부 파일 목록
                                        file_blobs = bucket.list_blobs(prefix=stock_prefix, delimiter='/')
                                        for file_blob in file_blobs:
                                            file_name = file_blob.name[len(stock_prefix):]
                                            if not file_name.strip():
                                                continue

                                            print(f"DEBUG: stock_exch_contry_code={country_code}, crawled_date={date_code}, stock_issue_code={stock_code}, file_name={file_name}")

                                            sql = """
                                                INSERT IGNORE INTO pdf_bucket_file_table
                                                (stock_exch_contry_code, crawled_date, stock_issue_code, file_name)
                                                VALUES (%s, %s, %s, %s)
                                            """
                                            try:
                                                cursor.execute(sql, (country_code, date_code, stock_code, file_name))
                                            except Exception as e:
                                                print(f"ERROR: SQL execution failed for {country_code}/{date_code}/{stock_code}/{file_name} - {str(e)}")
                connection.commit()
                print("DEBUG: DB commit successful")
        except Exception as e:
            print(f"ERROR: DB cursor operation failed - {str(e)}")
            raise
        finally:
            connection.close()
            print("DEBUG: DB connection closed")
    except Exception as e:
        print(f"ERROR: Failed in list_and_store_files - {str(e)}")
        raise

if __name__ == "__main__":
    BUCKET_NAME = os.environ.get("BUCKET_NAME")
    if not BUCKET_NAME:
        raise ValueError("BUCKET_NAME 환경변수가 설정되어 있지 않습니다.")
    try:
        list_and_store_files(BUCKET_NAME, "pdf_export/")
        print("pdf_export 하위 파일 정보를 DB에 저장했습니다.")
    except Exception as e:
        print(f"ERROR: 프로그램 실행 중 오류 발생 - {str(e)}")
```



> fn-calldb-bucket-file-list

```py
import pymysql
import os
from flask import jsonify

DB_HOST = os.environ.get('DB_HOST')
DB_USER = os.environ.get('DB_USER')
DB_PASSWORD = os.environ.get('DB_PASSWORD')
DB_NAME = os.environ.get('DB_NAME')

CUSTOM_DOMAIN = "https://callfn.wocheon.site"  # 실제 배포 도메인으로 변경 (HTTPS)

def get_db_connection():
    return pymysql.connect(
        host=DB_HOST,
        user=DB_USER,
        password=DB_PASSWORD,
        db=DB_NAME,
        cursorclass=pymysql.cursors.DictCursor
    )

def cloud_function_handler(request):
    try:
        connection = get_db_connection()
        with connection.cursor() as cursor:
            cursor.execute("""
                SELECT crawl_data_id, stock_exch_contry_code, crawled_date, stock_issue_code, file_name 
                FROM pdf_bucket_file_table
            """)
            results = cursor.fetchall()
        connection.close()

        files = {}
        for row in results:
            crawl_data_id = row['crawl_data_id']
            country_code = row['stock_exch_contry_code']
            crawled_date = row['crawled_date']
            stock_code = row['stock_issue_code']
            file_name = row['file_name']

            url_path = f"{country_code}/{crawled_date}/{stock_code}/{file_name}"

            files[file_name] = {
                "url": f"{CUSTOM_DOMAIN}/download/{url_path}",
                "crawl_data_id": crawl_data_id,
                "stock_exch_contry_code": country_code,
                "crawled_date": crawled_date,
                "stock_issue_code": stock_code,
                "file_name": file_name
            }

        return jsonify({
            "status": "ok",
            "deployed_version": "2025-08-29-deploy-test",
            "files": files
        })
    except Exception as e:
        return jsonify({"error": str(e)}), 500
```

> fn-calldb-bucket-file-url-redirect

```py
from flask import redirect
from google.cloud import storage, secretmanager
from google.oauth2 import service_account
import datetime
import os
import json

project_id = os.environ.get('PROJECT_ID')
secret_id = os.environ.get('SECRET_ID')
BUCKET_NAME = os.environ.get('BUCKET_NAME')
BUCKET_ACCESS_MODE = os.environ.get('BUCKET_ACCESS_MODE', 'PRIVATE').upper()

def access_secret_version(project_id, secret_id, version_id="latest"):
    client = secretmanager.SecretManagerServiceClient()
    name = f"projects/{project_id}/secrets/{secret_id}/versions/{version_id}"
    response = client.access_secret_version(name=name)
    secret_string = response.payload.data.decode("UTF-8")
    return secret_string

def generate_signed_url(blob_name, expiration_minutes=15):
    key_json_str = access_secret_version(project_id, secret_id)
    credentials_info = json.loads(key_json_str)
    credentials = service_account.Credentials.from_service_account_info(credentials_info)
    storage_client = storage.Client(credentials=credentials)
    bucket = storage_client.bucket(BUCKET_NAME)
    blob = bucket.blob(blob_name)
    url = blob.generate_signed_url(
        version="v4",
        expiration=datetime.timedelta(minutes=expiration_minutes),
        method="GET",
        credentials=credentials 
    )
    return url

def generate_public_url(blob_name):
    return f"https://storage.googleapis.com/{BUCKET_NAME}/{blob_name}"

def cloud_function_handler(request):
    try:
        path = request.path
        # 요청 URL에서 국가코드, 일자, 주식코드, 파일명 추출
        # 예: /download/<country_code>/<date>/<stock_code>/<filename>
        parts = path.split('/')
        if len(parts) < 6 or parts[1].lower() != 'download':
            return "Invalid URL path", 400

        country_code = parts[2]
        crawled_date = parts[3]
        stock_code = parts[4]
        file_name = parts[5]

        # GCS blob 경로 생성
        blob_name = f"pdf_export/{country_code}/{crawled_date}/{stock_code}/{file_name}"

        if BUCKET_ACCESS_MODE == 'PUBLIC':
            redirect_url = generate_public_url(blob_name)
        else:
            redirect_url = generate_signed_url(blob_name)

        return redirect(redirect_url)
    except Exception as e:
        return f"Error generating download URL: {e}", 500
