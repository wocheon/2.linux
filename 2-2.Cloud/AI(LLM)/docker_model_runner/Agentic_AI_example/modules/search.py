# modules/search.py (ì „ì²´ ì½”ë“œ ì—…ë°ì´íŠ¸)

import logging
import traceback
import config
from core.llm_client import get_client

# íŒ¨í‚¤ì§€ëª… í˜¸í™˜ì„± ì²˜ë¦¬
try:
    from ddgs import DDGS
except ImportError:
    import warnings
    warnings.filterwarnings("ignore", message=".*renamed to `ddgs`.*", category=RuntimeWarning)
    from duckduckgo_search import DDGS

logger = logging.getLogger(__name__)
client = get_client()

# [ìˆ˜ì •] í•¨ìˆ˜ëª…ì„ run_searchë¡œ ë³€ê²½
def run_search(user_input, context_messages=None):
    """
    [SEARCH] ë‹¨ìˆœ ì •ë³´/íŒ©íŠ¸ ê²€ìƒ‰ (ë¬¸ë§¥ ë°˜ì˜)
    """
    
    # 1. ê²€ìƒ‰ì–´ ìƒì„± (ë¬¸ë§¥ ë°˜ì˜)
    yield "ğŸ” ë¬¸ë§¥ì„ íŒŒì•…í•˜ì—¬ ì •ë³´ë¥¼ ì°¾ê³  ìˆìŠµë‹ˆë‹¤...\n"
    
    search_query = user_input
    
    # ë¬¸ë§¥ì´ ìˆìœ¼ë©´ LLMì—ê²Œ ê²€ìƒ‰ì–´ ìƒì„± ìš”ì²­
    if context_messages:
        system_prompt = """
        You are a Query Refiner.
        Rewrite the user's question into a specific search query for factual information.
        - Resolve pronouns (e.g., "it", "that") using history.
        - Output ONLY the query string.
        """
        
        messages = [{"role": "system", "content": system_prompt}]
        messages.extend(context_messages[-4:])
        messages.append({"role": "user", "content": user_input})

        try:
            resp = client.chat.completions.create(
                model=config.FAST_MODEL_NAME,
                messages=messages, temperature=0
            )
            rewritten = resp.choices[0].message.content.strip().replace('"', '')
            if len(rewritten) < 100:
                search_query = rewritten
                logger.info(f"Query Rewritten: {search_query}")
                yield f"ğŸ” ìµœì í™”ëœ ê²€ìƒ‰ì–´: '{search_query}'\n"
        except:
            pass

    # 2. ê²€ìƒ‰ ìˆ˜í–‰
    yield f"ğŸŒ ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤... ('{search_query}')\n"
    
    try:
        with DDGS() as ddgs:
            results = list(ddgs.text(search_query, region='kr-kr', timelimit='y', max_results=5))
    except Exception as e:
        yield f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}"
        return

    if not results:
        yield "âŒ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        return

    # 3. ë‹µë³€ ìƒì„± (íŒ©íŠ¸ ìœ„ì£¼)
    yield "ğŸ“Š ì •ë³´ë¥¼ ë¶„ì„í•˜ì—¬ ë‹µë³€ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤...\n\n"
    
    formatted_results = "\n".join([f"- {r['title']}: {r['body']}" for r in results])
    
    system_prompt = """
    You are a Fact Search Assistant.
    Answer based on [Search Results].
    - Focus on factual info (specs, price, dates, names).
    - Answer in Korean.
    - Be concise and clear.
    """
    
    try:
        stream = client.chat.completions.create(
            model=config.SMART_MODEL_NAME,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Question: {user_input}\n\nResults:\n{formatted_results}"}
            ],
            stream=True, temperature=0.3
        )
        for chunk in stream:
            if chunk.choices[0].delta.content: yield chunk.choices[0].delta.content
            
    except Exception as e:
        yield f"ì˜¤ë¥˜ ë°œìƒ: {e}"
