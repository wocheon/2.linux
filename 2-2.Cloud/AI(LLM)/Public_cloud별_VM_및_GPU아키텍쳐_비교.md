
# 클라우드 3사 VM 및 GPU 아키텍처 비교 화이트페이퍼  

## 개요

이 문서는 2025년 기준 퍼블릭 클라우드 3사(AWS, Azure, GCP)가 사용하는 **가상 머신(VM) 할당 구조**,  
**GPU 연결 방식**, **하이퍼바이저 아키텍처**, **커스터마이징 지원 정책**을 비교한다.  
AI 워크로드, HPC, 개발/운영 환경에서의 VM 선택 및 확장 전략 수립에 참고할 수 있다.

***

## 1. VM 할당 구조

| 항목 | AWS | Azure | GCP |
|------|------|-------|------|
| 리소스 관리 모델 | 고정 Slot 기반 | VM Size 기반 | Resource Pool 기반 |
| 스케줄러 엔진 | EC2 Placement Manager | Azure Fabric Controller | Borg / Omega |
| CPU·RAM 조합 | 인스턴스 타입별 고정 비율 | VM Size별 고정 비율 | 완전한 조합 지원 (커스텀 머신타입) |
| 확장 방식 | ASG 기반 수평 확장 | VM Scale Set | Compute Engine / GKE 오토스케일링 |
| 스케줄링 단위 | 인스턴스 단위 | Size 슬롯 단위 | 개별 자원 단위 (vCPU, RAM, GPU) |

```
AWS/Azure: [Fixed Slot 방식]
 └─ 하나의 물리 호스트는 동일 사양의 VM만 수용 가능

GCP: [Resource Pool 방식]
 └─ CPU, RAM, GPU는 추상화된 단위로 풀링되며 자유롭게 조합 가능
```

***

## 2. 하이퍼바이저 아키텍처 비교

| 구분 | AWS | Azure | GCP |
|------|------|-------|------|
| 하이퍼바이저 이름 | Nitro Hypervisor | Hyper‑V | KVM (Borg Scheduler 통합) |
| 가상화 계층 | 하드웨어 기반 (Nitro Card, 최소 OS 오버헤드) | 전통적 Hyper‑V 계층 (SR-IOV 지원) | 소프트웨어 기반 KVM + SDN |
| 핵심 설계 목표 | 보안 격리, 물리 성능 일관성 | SLA 기반 격리, 기업형 관리 | 유연성, 경량 스케줄링 |
| 관리 구성 | EC2 노드별 고정 구조 | Fabric Controller에 의해 VM Size별 관리 | 리소스 단위 동적 배치 (Borg) |
| 리소스 통합 수준 | 낮음 (Slot 단위 고정) | 중간 (VM 크기 단위) | 높음 (리소스 단위 통합) |
| 커널 마이그레이션 | 불가 (하드웨어 패스스루) | 제한적 | 가능 (CPU·RAM 한정, GPU 제외) |

***

## 3. GPU 연결 및 가상화 구조

| 항목 | AWS | Azure | GCP |
|------|------|-------|------|
| GPU 연결 방식 | PCIe Passthrough (물리 직결) | SR‑IOV Direct Assignment | PCIe Direct Passthrough (Borg 관리) |
| GPU 공유 | MIG (A100/H100) 지원 | NVv4(vGPU) 한정 | MIG 및 vGPU Profiles 일부 |
| GPU Time‑Sharing | 미지원 | 제한적 (VDI용) | 내부 Scheduling 구현 일부 리전 한정 |
| Live Migration | 불가 | 불가 (유지보수 연기 가능) | 불가 (자동 재시작) |
| 유지보수 시 동작 | 사용자가 직접 Event 대응 | Maintenance Control 설정 가능 | Maintenance Policy 무시 후 자동 중단 가능 |
| 대표 GPU VM | p4/p5/g5/g6 | ND/NV/NC 시리즈 | a2/g2/n1/n2 |

***

## 4. GPU 가상화 기술별 비교

| 기술 | 동작 방식 | 장점 | 단점 | 채택 현황 |
|------|------------|------|------|-------------|
| Passthrough | GPU 전체를 단일 VM에 직결 | 최대 성능, 단순 구성 | 유지보수 중단 필요 | 전체 공통 |
| MIG (Multi‑Instance GPU) | GPU를 물리적으로 분할 | 성능 손실 거의 없음 | 구성 복잡 | 지원 확대 중 |
| vGPU | GPU를 논리 파티션(소프트웨어)으로 분할 | 공유 활용도 높음 | 성능 저하, 라이선스 필요 | Azure NVv4, GCP Workstation |
| Time‑Sharing | GPU 컨텍스트를 스케줄링 레벨에서 분할 | 높은 자원 효율 | 지연, 충돌 발생율 높음 | GCP 일부 내부 적용 |

***

## 5. VM 및 GPU 커스터마이징 지원

| 항목 | AWS | Azure | GCP |
|------|------|-------|------|
| vCPU 커스터마이징 | 코어 비활성 옵션 가능 (`--cpu-options`) | VM Size 내 제한적 조정 | 완전 커스텀 (任의 개수) |
| RAM 커스터마이징 | 불가 | 불가 | 완전 커스텀 |
| GPU 개수 선택 | 일부 시리즈(A10/L4)만 | 일부 시리즈(vGPU)만 | 1~16개 모두 선택 가능 |
| 커스텀 머신 지원 | 미지원 | 일부 리전 시험 중 | 기본 제공 |
| 하드웨어 고정성 | 높음 (Slot 기반) | 높음 (Fabric Slot) | 낮음 (Dynamic Pool) |

***

## 6. 유지보수 정책 비교

| 항목 | AWS | Azure | GCP |
|------|------|-------|------|
| 유지보수 이벤트 제어 | EC2 Scheduled Event로 수동 제어 | Maintenance Control로 35일 연기 가능 | 자동 수행 (사용자 제어 불가) |
| GPU Live Migration | 미지원 | 미지원 | 미지원 |
| 중단 방지 정책 | 사용자가 Event 확인 후 재시작 가능 | 특정 VM 그룹만 연기 가능 | GKE GPU 노드는 무시 불가 |
| 사용성 평가 | 수동 대응 유연 | 관리 편의 높음 | 운영 리스크 존재 |

***

## 7. 커스터마이징 미지원 이유 (AWS·Azure vs GCP의 가능 구조)

### AWS·Azure가 커스터마이징을 제공하지 않는 이유

AWS와 Azure는 기술적으로 하이퍼바이저·리소스 관리자 입장에서 CPU 및 메모리 조정이 가능하지만,  
서비스 구조 및 비즈니스 운영 단위에서는 아래와 같은 이유로 **의도적으로 기능을 제한**한다.

1. **물리 슬롯 기반 리소스 관리**  
   - AWS의 EC2 및 Azure의 VM Size는 물리 서버가 동일한 사양(SKU)의 VM만 수용하도록 미리 슬롯이 구성되어 있다.  
   - 이러한 고정형 구조는 유지보수 시 빠른 교체(Instance Refresh)와 자원 예측이 가능하지만,  
     서로 다른 CPU·RAM 비율의 VM이 섞이면 NUMA 구조 및 자원 Scheduling 불균형이 발생한다.  
   - 즉, 커스터마이징을 허용하면 한 물리 호스트에서 자원 할당이 비대칭해지고 배치 효율이 급격히 저하된다.

2. **청구 및 운영 단순화**  
   - AWS는 "인스턴스 타입별 요금"이라는 단일 정가 모델을 유지한다.  
   - 만약 커스텀 조합을 허용하면 수천 가지 CPU·RAM 조합이 생겨 청구서, Savings Plans, Reserved Instance 체계가 복잡해진다.  
   - Azure 역시 VM Size 단위의 라이선스 및 OS SKU를 기반으로 가격을 책정하기 때문에 커스텀 구성은 회계적으로 불리하다.  

3. **SLA 품질 보장과 성능 예측성**  
   - 모든 VM 타입은 AWS/Azure 내부에서 성능 검증 프로파일(Performance Baseline)을 가지고 있다.  
   - 사용자 커스텀 조합은 이러한 검증 범위를 벗어나므로, VM 간 성능 편차와 예측 불가능성을 초래한다.  
   - 따라서 “SLA 보장” 관점에서 커스텀을 지원하지 않는 것이 더 안정적이다.

4. **NUMA 및 하드웨어 구성 고정**  
   - 대부분의 클라우드 물리 서버는 NUMA(Node)별 CPU·RAM 채널 구성이 고정되어 있다.  
   - 예를 들어, 96 vCPU / 384GB RAM 구조라면 1:4 비율로만 나누어야 메모리 접근지연이 최소화된다.  
   - 이를 임의로 조정하면 메모리 대역폭 불균형이 발생해 VM당 성능 차이가 커지므로, 클라우드는 이를 구조적으로 금지한다.

결국, AWS와 Azure는 하드웨어 격리(HW-level Isolation)와 운영 효율(OpEx Reduction)을 우선시하기 때문에  
`“기술적으로 가능하지만, 운영 상 비효율적이어서 비공개”` 인 기능으로 남아 있다.  
두 플랫폼 모두 커스터마이징 대신 수십~수백 개의 세분화된 인스턴스 패밀리(Type/Size)를 제공하여 이를 보완한다.

***

### 그럼 GCP는 왜 가능한가?

GCP가 커스터마이징을 제공할 수 있는 이유는 **플랫폼 아키텍처 자체가 AWS/Azure와 다르게 설계되어 있기 때문**이다.

1. **리소스 풀 기반 구조 (Resource Pool Architecture)**  
   - GCP는 VM을 생성할 때 CPU, 메모리, GPU를 하드웨어 슬롯에 직접 매핑하지 않고,  
     “리소스 풀(Resource Pool)” 단위로 추상화하여 관리한다.  
   - Borg/Omega 스케줄러가 해당 풀에서 필요한 vCPU와 RAM을 논리적으로 조합해 VM을 작성하므로,  
     하드웨어 종속성이 적고 다양한 비율의 조합이 가능하다.

2. **유연한 소프트웨어 스케줄링 (Software-defined Allocation)**  
   - AWS·Azure는 클러스터 노드가 하드웨어 슬롯 정렬형(Uniform Instance Block)으로 관리되지만,  
     GCP는 “소프트웨어 오케스트레이터”가 자원 배치를 가상화한다.  
   - 이 때문에 CPU 3개, RAM 17GB와 같은 비정형 조합도 스케줄러가 알아서 최적화해 할당할 수 있다.  

3. **자원 조합-단위 요금 모델**  
   - GCP는 인스턴스 타입 개념 대신 “vCPU/GB 단위 요금제”를 사용한다.  
   - 덕분에 CPU와 RAM 비율을 자유롭게 변경해도 실제 가격 계산이 가능하며,  
     이는 청구 체계 자체가 커스터마이징을 지원하도록 설계된 구조이다.

4. **하이퍼바이저의 유연성 (KVM + Andromeda SDN)**  
   - GCP의 KVM 기반 하이퍼바이저는 소프트웨어 정의 네트워킹(Andromeda) 및 VM 스케줄링이 완전 통합되어 있다.  
   - 같은 물리 노드에서도 VM 간 자원을 논리적으로 분리할 수 있어,  
     “같은 호스트에서 다른 스펙의 커스텀 VM”을 동시에 실행할 수 있다.

5. **검증 방식 차이**  
   - GCP는 SLA를 VM 단위가 아닌 “자원 풀 가용성” 기준으로 평가한다.  
   - 따라서 VM 사양이 세분화되어도 일관된 SLA 적용이 가능하므로,  
     정책적으로 커스터마이징을 제공하기 용이하다.

결국 GCP는 **자원 추상화 수준이 더 높고, 청구 시스템이 단위 과금 기반으로 설계되어 있어 커스터마이징을 실질적으로 구현 가능**하다.  
반대로 AWS·Azure는 초기 설계에서 물리 노드를 균일하게 관리하도록 구성했기 때문에,  
커스터마이징을 열어버리면 스케줄링/비용체계/SLA 전반이 붕괴되는 구조이다.

***

### 정리 비교
- **AWS·Azure:** 커스터마이징은 기술적으로 가능하지만, 운영 효율성과 SLA·비용·배포 속도 문제로 일부러 봉인.  
- **GCP:** 플랫폼 레벨에서 자원을 가상화해 관리하므로 CPU·RAM·GPU 조합을 자유롭게 제공 가능.  
- 즉, GCP의 커스터마이징은 “기술적 우위”가 아니라 “설계 철학 차이”의 결과
	- AWS·Azure는 “안정적 균일성”, GCP는 “유연한 조합성”을 우선하는 구조라고 볼 수 있다.

| 구분 | AWS | Azure | GCP |
|------|------|-------|------|
| 자원 할당 단위 | 고정 Slot (Instance Type) | 고정 VM Size (SKU별) | 동적 Resource Pool |
| 하이퍼바이저 | Nitro (HW Offload) | Hyper‑V | KVM (SW Defined) |
| 커스터마이징 지원 기술적으로 가능? | 가능 | 가능 | 가능 |
| 운영 구조적으로 허용? | 불가능 | 제한적 | 완전 가능 |
| 제한 이유 | 성능 일관성 / NUMA 구조 / 청구 단순화 | SLA 유지 / Fabric Controller 구속 | 없음 (동적 스케줄링) |
| 사용자 조합 가능 여부 | vCPU 제한 옵션 수준 | VM Size 내부 자동 조정 | 완전 조합 (Custom Machine Type) |

***

## 8. 클라우드 설계 철학 비교

| 관점 | AWS | Azure | GCP |
|------|------|-------|------|
| 중심 가치 | 안정성, 일관된 성능, 보안 | 관리 자동화, SLA 중심 | 유연성, 비용 효율 |
| 하이퍼바이저 방향 | HW 기반 최소 계층 | 엔터프라이즈 친화형 구조 | 소프트웨어 기반 추상화 |
| GPU 전략 | 물리 성능 위주 (MIG 확장) | 엔터프라이즈 시각화 병용 | 연구/AI 유연 구조 |
| 커스터마이징 | 제한 (옵션형) | 중간 (Flexible VM Size) | 완전 지원 |
| 유지보수 접근 | 사용자 주도 | 예약/연기형 | 자동화 중심 |

***

## 9. 구조 시각화 개요

```
──────────────────────────────────────────────
 AWS (Nitro System)
──────────────────────────────────────────────
 [Physical Host]
   ├─ Nitro Hypervisor (HW 가상화)
   ├─ PCIe GPU Direct 연결
   └─ Fixed Instance Slot (vCPU, RAM 비율 고정)
   → 안정성, 보안, 고정형 구조

──────────────────────────────────────────────
 Azure (Hyper‑V + Fabric Controller)
──────────────────────────────────────────────
 [Physical Host]
   ├─ Hyper‑V + SR‑IOV 가상화
   ├─ GPU Direct Assignment
   └─ VM Size 슬롯 단위 관리 (Fabric)
   → 관리 자동화, SLA 중심, 커스터마이징 제한

──────────────────────────────────────────────
 GCP (KVM + Borg Scheduler)
──────────────────────────────────────────────
 [Physical Host]
   ├─ KVM Hypervisor + Andromeda SDN
   ├─ Resource Pool: CPU·RAM·GPU 단위 분리
   └─ Borg Scheduler가 동적 조합
   → 유연성, 커스텀 머신타입, Dynamic GPU
```

***

## 10. 결론

- **AWS:** 안정성과 예측 가능한 성능 중심, 하드웨어 기반 Nitro 덕분에 오버헤드 없음. 대신 유연성은 낮음.  
- **Azure:** Hyper‑V 기반 관리형 아키텍처로, 유지보수 제어성이 높지만 리소스 구조가 정형화되어 있음.  
- **GCP:** 리소스 풀 단위 추상화 덕분에 CPU, RAM, GPU 커스터마이징이 가능. 단 GPU Live Migration 제약 존재.

***

## 부록 A. GPU 가상화 발전 방향 요약

| 세대 | 기술 | 설명 | 사용 예시 |
|------|--------|--------|------------|
| 1세대 | Passthrough | GPU 1:1 직결 | 대부분의 GPU VM |
| 2세대 | vGPU | 소프트웨어 논리 분할 | VDI, 경량 추론용 |
| 3세대 | MIG | GPU 하드웨어 파티션 | A100/H100 학습 환경 |
| 4세대 (예정) | SR‑IOV for GPU | OS 레벨 위 가상화 표준화 (NVIDIA Blackwell 등) | 차세대 클라우드용 GPU 인스턴스 |

향후 대규모 AI 인프라에서는 **GPU SR‑IOV** 방식을 도입해,  
Passthrough 성능과 vGPU의 유연성을 동시에 달성하는 방향으로 진화할것으로 예상됨



## 부록 B. 관점 별 Public Cloud간 비교 

| 평가 관점                 | 1위                            | 2위    | 3위    | 비고                                     |
| --------------------- | ----------------------------- | ----- | ----- | -------------------------------------- |
| 💪 **절대 성능 중심**       | **AWS (Nitro)**               | GCP   | Azure | Nitro의 HW 오프로딩 덕분에 CPU/I/O 오버헤드가 거의 없음 |
| 🧱 **보안·격리 중심**       | **AWS (Nitro Security Chip)** | Azure | GCP   | AWS는 하이퍼바이저 자체를 HW칩으로 분리함              |
| ⚙️ **운영 효율·자원 활용 중심** | **GCP (Borg + KVM)**          | AWS   | Azure | GCP는 자원을 컨테이너 단위로 스케줄링 가능              |
| 🔧 **커스터마이징 자유도 중심**  | **GCP**                       | Azure | AWS   | GCP만 커스텀 vCPU/메모리 설정 가능                |
| 💰 **비용 유연성 중심**      | **GCP**                       | Azure | AWS   | 정확히 사용한 리소스만큼 과금 (커스텀 VM)              |
| 🏢 **엔터프라이즈 통합성 중심**  | **Azure**                     | AWS   | GCP   | AD/Windows 환경과 통합성이 압도적                |
