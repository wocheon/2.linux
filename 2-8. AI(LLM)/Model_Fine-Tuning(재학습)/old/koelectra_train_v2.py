import configparser
import os
import torch
import numpy as np
import shutil
from datasets import load_dataset
from transformers.trainer_utils import get_last_checkpoint
from transformers import (
    ElectraTokenizer,
    ElectraForSequenceClassification,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding,
    TrainerCallback
)
from sklearn.metrics import accuracy_score, f1_score
import gc

# WANDB ë¹„í™œì„±í™” (ë¡œê·¸ì¸ ìš”êµ¬ ë°©ì§€)
os.environ["WANDB_DISABLED"] = "true"
# í† í¬ë‚˜ì´ì € ë³‘ë ¬ ì²˜ë¦¬ ê²½ê³  ë„ê¸°
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ==========================================
# 0. ìœ í‹¸ë¦¬í‹° & ì½œë°± í´ë˜ìŠ¤
# ==========================================
def get_data_format(file_path):
    """íŒŒì¼ í™•ì¥ìë¥¼ ê¸°ë°˜ìœ¼ë¡œ load_datasetì˜ í¬ë§· íŒŒë¼ë¯¸í„°ë¥¼ ê²°ì •"""
    ext = file_path.split('.')[-1].lower()
    if ext == 'csv': return 'csv'
    elif ext in ['json', 'jsonl']: return 'json'
    elif ext == 'parquet': return 'parquet'
    else:
        print(f"âš  ì•Œ ìˆ˜ ì—†ëŠ” í™•ì¥ì(.{ext})ì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ 'csv'ë¡œ ì‹œë„í•©ë‹ˆë‹¤.")
        return 'csv'

class DescriptionCallback(TrainerCallback):
    """í•™ìŠµ ë¡œê·¸ë¥¼ ë” ì¹œì ˆí•˜ê²Œ ì¶œë ¥í•˜ëŠ” ì»¤ìŠ¤í…€ ì½œë°±"""
    def on_log(self, args, state, control, logs=None, **kwargs):
        if state.is_local_process_zero and logs:
            epoch_info = f"[Epoch: {logs.get('epoch', 0):.2f}]"
            if 'loss' in logs:
                loss_val = logs['loss']
                if loss_val > 0.8: comment = "ğŸ˜° ì•„ì§ í—¤ë§¤ëŠ” ì¤‘..."
                elif loss_val > 0.5: comment = "ğŸ¤” ê°ì„ ì¡ëŠ” ì¤‘..."
                elif loss_val > 0.3: comment = "ğŸ™‚ í•™ìŠµì´ ì˜ ë˜ê³  ìˆì–´ìš”!"
                else: comment = "ğŸš€ ì™„ë²½í•´ìš”!"
                print(f"{epoch_info} Loss: {loss_val:.4f} -> {comment}")
            if 'learning_rate' in logs:
                print(f"   â””â”€ LR: {logs['learning_rate']:.2e}")

# ==========================================
# 1. Config ë¡œë“œ
# ==========================================
config = configparser.ConfigParser()
config_path = 'config.ini'

if not os.path.exists(config_path):
    print(f"âš  ê²½ê³ : '{config_path}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    config['Path'] = {
        'model_name': 'klue/roberta-base',
        'train_file': 'dataset.csv',
        'output_dir': './output',
        'checkpoint_dir': './checkpoints'
    }
    config['Hyperparameters'] = {
        'num_labels': '3',
        'max_seq_length': '128',
        'batch_size': '64',
        'learning_rate': '2e-5',
        'epochs': '3',
        'seed': '42',
        'split_ratio': '0.2',
        'min_confidence': '0.8' # [New] ìµœì†Œ ì‹ ë¢°ë„ ì„¤ì •
    }
else:
    config.read(config_path)

# [Path] ì„¹ì…˜ ë¡œë“œ
MODEL_NAME = config['Path']['model_name']
TRAIN_FILE = os.path.abspath(config['Path']['train_file'])
VALID_FILE_RAW = config['Path'].get('valid_file', '').strip()
VALID_FILE = os.path.abspath(VALID_FILE_RAW) if VALID_FILE_RAW else ""
OUTPUT_DIR = config['Path']['output_dir']
CHECKPOINT_DIR = config['Path']['checkpoint_dir']

# [Hyperparameters] ì„¹ì…˜ ë¡œë“œ
params = config['Hyperparameters']

NUM_LABELS = int(params.get('num_labels', 3))
MAX_LEN = int(params.get('max_seq_length', 128))
TARGET_BATCH_SIZE = int(params.get('batch_size', 64))
LR = float(params.get('learning_rate', 2e-5))
EPOCHS = int(params.get('epochs', 3))
SEED = int(params.get('seed', 42))
SPLIT_RATIO = float(params.get('split_ratio', 0.2))
MIN_CONFIDENCE = float(params.get('min_confidence', 0.8)) # [New] í•„í„°ë§ ê¸°ì¤€

# [Subset] ì˜µì…˜
USE_SUBSET = config.getboolean('Hyperparameters', 'use_subset', fallback=False)
SUBSET_SIZE = config.getint('Hyperparameters', 'subset_size', fallback=100)

print(f"â–¶ ëª¨ë¸: {MODEL_NAME}")
print(f"â–¶ í•™ìŠµ íŒŒì¼: {TRAIN_FILE}")

# ==========================================
# 2. ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ (ìˆ˜ì •ë¨)
# ==========================================
data_format = get_data_format(TRAIN_FILE)
print(f"â–¶ ê°ì§€ëœ í¬ë§·: {data_format}")

# ì›ë³¸ ë°ì´í„° ë¡œë“œ
raw_dataset = load_dataset(data_format, data_files={"train": TRAIN_FILE})['train']

# -------------------------------------------------------------
# [Step A] ë°ì´í„° í•„í„°ë§ ë° í¬ë§·íŒ…
# -------------------------------------------------------------
def filter_and_format(example):
    # 1. Confidence ì²´í¬ (ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°ë§Œ)
    if 'confidence' in example and example['confidence'] is not None:
        try:
            if float(example['confidence']) < MIN_CONFIDENCE:
                return False
        except:
            pass # íŒŒì‹± ì—ëŸ¬ ì‹œ ì•ˆì „í•˜ê²Œ ìœ ì§€ í˜¹ì€ ì œê±°(ì—¬ê¸°ì„  ìœ ì§€)
            
    # 2. Label ìœ íš¨ì„± ì²´í¬ (0, 1, 2ë§Œ í—ˆìš©)
    if example.get('label_sentiment') not in [0, 1, 2]:
        return False
        
    return True

print(f"ğŸ“‰ í’ˆì§ˆ í•„í„°ë§ ì „: {len(raw_dataset)}ê±´")
filtered_dataset = raw_dataset.filter(filter_and_format)
print(f"ğŸ“ˆ í’ˆì§ˆ í•„í„°ë§ í›„: {len(filtered_dataset)}ê±´ (ê¸°ì¤€: conf >= {MIN_CONFIDENCE})")

# -------------------------------------------------------------
# [Step B] ì…ë ¥ í…ìŠ¤íŠ¸ ì¡°í•© (í…Œë§ˆ ì œì™¸)
# -------------------------------------------------------------
def combine_text(example):
    tgt = example.get('target', 'ì‹œì¥')
    # í…Œë§ˆ(themes)ëŠ” ì¶”ë¡  ì‹œ ì˜ì¡´ì„± ë¬¸ì œë¡œ ì œì™¸í•©ë‹ˆë‹¤.
    title = example.get('title', '')
    
    # [Target] Title í˜•ì‹ìœ¼ë¡œ ê²°í•©
    combined_text = f"[{tgt}] {title}"
    
    return {
        "text": combined_text, 
        "label": int(example['label_sentiment']) 
    }

# ë°ì´í„° ë³€í™˜ ì ìš© (ê¸°ì¡´ ì»¬ëŸ¼ ì œê±°)
processed_dataset = filtered_dataset.map(combine_text, remove_columns=filtered_dataset.column_names)

# í•™ìŠµ/ê²€ì¦ ë¶„í• 
if VALID_FILE and os.path.exists(VALID_FILE):
    # ê²€ì¦ íŒŒì¼ë„ ë™ì¼í•˜ê²Œ ì „ì²˜ë¦¬ í•„ìš”
    raw_eval = load_dataset(data_format, data_files={"validation": VALID_FILE})['validation']
    eval_dataset = raw_eval.filter(filter_and_format).map(combine_text, remove_columns=raw_eval.column_names)
    train_dataset = processed_dataset
else:
    if len(processed_dataset) < 10:
        raise ValueError("âŒ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤. ìµœì†Œ 10ê°œ ì´ìƒ í•„ìš”í•©ë‹ˆë‹¤.")
    split_datasets = processed_dataset.train_test_split(test_size=SPLIT_RATIO, seed=SEED)
    train_dataset = split_datasets["train"]
    eval_dataset = split_datasets["test"]

if USE_SUBSET:
    print(f"\nâš  [í…ŒìŠ¤íŠ¸ ëª¨ë“œ] ë°ì´í„° ì¶•ì†Œ ì‹¤í–‰")
    if len(train_dataset) > SUBSET_SIZE:
        train_dataset = train_dataset.select(range(SUBSET_SIZE))
    eval_dataset = eval_dataset.select(range(min(len(eval_dataset), int(SUBSET_SIZE * 0.2))))

print(f"âœ… ìµœì¢… ì¤€ë¹„ ì™„ë£Œ: í•™ìŠµ({len(train_dataset)}) / ê²€ì¦({len(eval_dataset)})")
print(f"ğŸ‘€ ì…ë ¥ ì˜ˆì‹œ: {train_dataset[0]['text']} -> ë¼ë²¨: {train_dataset[0]['label']}")

# ==========================================
# 3. í† í¬ë‚˜ì´ì € ë¡œë“œ ë° íŒ¨ì¹˜
# ==========================================
print("â³ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
tokenizer = ElectraTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)

# [Monkey Patch] save_vocabulary í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°
if not hasattr(tokenizer, "_original_save_vocabulary"):
    if hasattr(tokenizer, "save_vocabulary"):
        tokenizer._original_save_vocabulary = tokenizer.save_vocabulary

def patched_save_vocabulary(save_directory, filename_prefix=None):
    if hasattr(tokenizer, "_original_save_vocabulary"):
        return tokenizer._original_save_vocabulary(save_directory)
    else:
        return ()

tokenizer.save_vocabulary = patched_save_vocabulary

# í† í¬ë‚˜ì´ì§• í•¨ìˆ˜
def preprocess_function(examples):
    return tokenizer(
        examples["text"], 
        truncation=True, 
        padding="max_length", 
        max_length=MAX_LEN
    )

tokenized_train = train_dataset.map(preprocess_function, batched=True)
tokenized_eval = eval_dataset.map(preprocess_function, batched=True)

# ==========================================
# 4. ëª¨ë¸ ë° í•™ìŠµ ì„¤ì •
# ==========================================
gc.collect()
torch.cuda.empty_cache()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"â–¶ í•™ìŠµ ì¥ì¹˜: {device}")

model = ElectraForSequenceClassification.from_pretrained(
    MODEL_NAME, 
    num_labels=NUM_LABELS,
    trust_remote_code=True,
    ignore_mismatched_sizes=True
)
model.to(device)

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    acc = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average="macro")
    return {"accuracy": acc, "f1": f1}

# OOM ë°©ì§€ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì„¤ì •
SAFE_GPU_BATCH_SIZE = 4 
if TARGET_BATCH_SIZE < SAFE_GPU_BATCH_SIZE:
    calculated_accum_steps = 1
    real_batch_size = TARGET_BATCH_SIZE
else:
    calculated_accum_steps = TARGET_BATCH_SIZE // SAFE_GPU_BATCH_SIZE
    real_batch_size = SAFE_GPU_BATCH_SIZE

print(f"ğŸ”§ [ì„¤ì •] ëª©í‘œ ë°°ì¹˜: {TARGET_BATCH_SIZE} -> ì‹¤ì œ: {real_batch_size} (ëˆ„ì  {calculated_accum_steps}íšŒ)")

training_args = TrainingArguments(
    output_dir=CHECKPOINT_DIR,
    learning_rate=LR,
    per_device_train_batch_size=real_batch_size,
    per_device_eval_batch_size=real_batch_size,
    gradient_accumulation_steps=calculated_accum_steps,
    fp16=True,
    num_train_epochs=EPOCHS,
    weight_decay=0.01,
    eval_strategy="epoch",
    save_strategy="epoch",    
    load_best_model_at_end=True,
    save_total_limit=2,
    seed=SEED,
    logging_dir=f"{OUTPUT_DIR}/logs",
    logging_steps=500,
    optim="adamw_torch"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_eval,
    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),
    compute_metrics=compute_metrics,
    callbacks=[DescriptionCallback()],
)

# ==========================================
# 5. í•™ìŠµ ì‹¤í–‰ ë° ì €ì¥
# ==========================================
print("ğŸš€ í•™ìŠµ ì‹œì‘...")
last_checkpoint = get_last_checkpoint(CHECKPOINT_DIR) if os.path.isdir(CHECKPOINT_DIR) else None

trainer.train(resume_from_checkpoint=last_checkpoint)

print(f"ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥: {OUTPUT_DIR}")
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
print("ğŸ‰ ì™„ë£Œ!")