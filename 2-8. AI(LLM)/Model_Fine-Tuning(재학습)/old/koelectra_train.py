import configparser
import os
import torch
import numpy as np
import shutil
from datasets import load_dataset
from transformers import (
    ElectraTokenizer,
    ElectraForSequenceClassification,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding,
    TrainerCallback
)
from sklearn.metrics import accuracy_score, f1_score
import gc

# WANDB ë¹„í™œì„±í™” (ë¡œê·¸ì¸ ìš”êµ¬ ë°©ì§€)
os.environ["WANDB_DISABLED"] = "true"
# í† í¬ë‚˜ì´ì € ë³‘ë ¬ ì²˜ë¦¬ ê²½ê³  ë„ê¸°
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ==========================================
# 0. ìœ í‹¸ë¦¬í‹° & ì½œë°± í´ë˜ìŠ¤
# ==========================================
def get_data_format(file_path):
    """íŒŒì¼ í™•ì¥ìë¥¼ ê¸°ë°˜ìœ¼ë¡œ load_datasetì˜ í¬ë§· íŒŒë¼ë¯¸í„°ë¥¼ ê²°ì •"""
    ext = file_path.split('.')[-1].lower()
    if ext == 'csv': return 'csv'
    elif ext in ['json', 'jsonl']: return 'json'
    elif ext == 'parquet': return 'parquet'
    else:
        print(f"âš  ì•Œ ìˆ˜ ì—†ëŠ” í™•ì¥ì(.{ext})ì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ 'csv'ë¡œ ì‹œë„í•©ë‹ˆë‹¤.")
        return 'csv'

class DescriptionCallback(TrainerCallback):
    """í•™ìŠµ ë¡œê·¸ë¥¼ ë” ì¹œì ˆí•˜ê²Œ ì¶œë ¥í•˜ëŠ” ì»¤ìŠ¤í…€ ì½œë°±"""
    def on_log(self, args, state, control, logs=None, **kwargs):
        if state.is_local_process_zero and logs:
            epoch_info = f"[Epoch: {logs.get('epoch', 0):.2f}]"
            if 'loss' in logs:
                loss_val = logs['loss']
                if loss_val > 0.8: comment = "ğŸ˜° ì•„ì§ í—¤ë§¤ëŠ” ì¤‘..."
                elif loss_val > 0.5: comment = "ğŸ¤” ê°ì„ ì¡ëŠ” ì¤‘..."
                elif loss_val > 0.3: comment = "ğŸ™‚ í•™ìŠµì´ ì˜ ë˜ê³  ìˆì–´ìš”!"
                else: comment = "ğŸš€ ì™„ë²½í•´ìš”!"
                print(f"{epoch_info} Loss: {loss_val:.4f} -> {comment}")
            if 'learning_rate' in logs:
                print(f"   â””â”€ LR: {logs['learning_rate']:.2e}")

# ==========================================
# 1. Config ë¡œë“œ
# ==========================================
config = configparser.ConfigParser()
config_path = 'config.ini'

if not os.path.exists(config_path):
    # íŒŒì¼ì´ ì—†ì„ ê²½ìš° ê¸°ë³¸ê°’ ìƒì„± (ê¸´ê¸‰ ë³µêµ¬ìš©)
    print(f"âš  ê²½ê³ : '{config_path}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    config['Path'] = {
        'model_name': 'klue/roberta-base',
        'train_file': 'dataset.csv',
        'output_dir': './output',
        'checkpoint_dir': './checkpoints'
    }
    config['Hyperparameters'] = {
        'num_labels': '3',
        'max_seq_length': '128',
        'batch_size': '64',
        'learning_rate': '2e-5',
        'epochs': '3',
        'seed': '42',
        'split_ratio': '0.2'
    }
else:
    config.read(config_path)

# [Path] ì„¹ì…˜ ë¡œë“œ
MODEL_NAME = config['Path']['model_name']
TRAIN_FILE = os.path.abspath(config['Path']['train_file'])
VALID_FILE_RAW = config['Path'].get('valid_file', '').strip()
VALID_FILE = os.path.abspath(VALID_FILE_RAW) if VALID_FILE_RAW else ""
OUTPUT_DIR = config['Path']['output_dir']
CHECKPOINT_DIR = config['Path']['checkpoint_dir']

# [Hyperparameters] ì„¹ì…˜ ë¡œë“œ (params ì •ì˜ í•„ìˆ˜!)
params = config['Hyperparameters']

NUM_LABELS = int(params.get('num_labels', 3))
MAX_LEN = int(params.get('max_seq_length', 128))
TARGET_BATCH_SIZE = int(params.get('batch_size', 64)) # ëª©í‘œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ
LR = float(params.get('learning_rate', 2e-5))
EPOCHS = int(params.get('epochs', 3))
SEED = int(params.get('seed', 42))
SPLIT_RATIO = float(params.get('split_ratio', 0.2))

# [Subset] ì˜µì…˜
USE_SUBSET = config.getboolean('Hyperparameters', 'use_subset', fallback=False)
SUBSET_SIZE = config.getint('Hyperparameters', 'subset_size', fallback=100)

print(f"â–¶ ëª¨ë¸: {MODEL_NAME}")
print(f"â–¶ í•™ìŠµ íŒŒì¼: {TRAIN_FILE}")

# ==========================================
# 2. ë°ì´í„°ì…‹ ë¡œë“œ
# ==========================================
data_format = get_data_format(TRAIN_FILE)
print(f"â–¶ ê°ì§€ëœ í¬ë§·: {data_format}")

if VALID_FILE and os.path.exists(VALID_FILE):
    print(f"â–¶ ê²€ì¦ íŒŒì¼: {VALID_FILE}")
    dataset = load_dataset(data_format, data_files={"train": TRAIN_FILE, "validation": VALID_FILE})
    train_dataset = dataset["train"]
    eval_dataset = dataset["validation"]
else:
    print(f"â–¶ ê²€ì¦ íŒŒì¼ ì—†ìŒ (í•™ìŠµ ë°ì´í„°ì—ì„œ {SPLIT_RATIO*100}% ìë™ ë¶„í• )")
    raw_dataset = load_dataset(data_format, data_files={"train": TRAIN_FILE})
    if len(raw_dataset["train"]) < 10:
        raise ValueError("âŒ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤. ìµœì†Œ 10ê°œ ì´ìƒ í•„ìš”í•©ë‹ˆë‹¤.")
    split_datasets = raw_dataset["train"].train_test_split(test_size=SPLIT_RATIO, seed=SEED)
    train_dataset = split_datasets["train"]
    eval_dataset = split_datasets["test"]

# ì»¬ëŸ¼ëª… í†µì¼ (Label, LABEL -> label)
for col in train_dataset.column_names:
    if col.lower() == 'label' and col != 'label':
        train_dataset = train_dataset.rename_column(col, "label")
        eval_dataset = eval_dataset.rename_column(col, "label")

if USE_SUBSET:
    print(f"\nâš  [í…ŒìŠ¤íŠ¸ ëª¨ë“œ] ë°ì´í„° ì¶•ì†Œ ì‹¤í–‰")
    if len(train_dataset) > SUBSET_SIZE:
        train_dataset = train_dataset.select(range(SUBSET_SIZE))
    eval_dataset = eval_dataset.select(range(min(len(eval_dataset), int(SUBSET_SIZE * 0.2))))

print(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: í•™ìŠµ({len(train_dataset)}) / ê²€ì¦({len(eval_dataset)})")

# ==========================================
# 3. í† í¬ë‚˜ì´ì € ë¡œë“œ ë° íŒ¨ì¹˜
# ==========================================
print("â³ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
try:
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
except Exception as e:
    print(f"âš  1ì°¨ ë¡œë“œ ì‹¤íŒ¨. ë¡œì»¬ í´ë” ë¬¸ì œì¼ ìˆ˜ ìˆì–´ ì¬ì‹œë„í•©ë‹ˆë‹¤... ì—ëŸ¬: {e}")
    # ë¡œì»¬ ê²½ë¡œ ë¬¸ì œ ì‹œ ê°•ì œ ì¬ë‹¤ìš´ë¡œë“œ ì˜µì…˜ ë“±ì„ ê³ ë ¤í•  ìˆ˜ ìˆìœ¼ë‚˜ ì—¬ê¸°ì„  íŒ¨ìŠ¤
    raise e

# [Monkey Patch] save_vocabulary í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°
if not hasattr(tokenizer, "_original_save_vocabulary"):
    if hasattr(tokenizer, "save_vocabulary"):
        tokenizer._original_save_vocabulary = tokenizer.save_vocabulary

def patched_save_vocabulary(save_directory, filename_prefix=None):
    if hasattr(tokenizer, "_original_save_vocabulary"):
        return tokenizer._original_save_vocabulary(save_directory)
    else:
        return ()

tokenizer.save_vocabulary = patched_save_vocabulary
print("ğŸ”§ í† í¬ë‚˜ì´ì € í˜¸í™˜ì„± íŒ¨ì¹˜ ì ìš© ì™„ë£Œ")

# ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_function(examples):
    col_candidates = ["text", "Text", "review", "content", "document", "Review_Text"]
    text_col = next((c for c in col_candidates if c in examples), None)
    if not text_col:
        # labelì´ ì•„ë‹Œ ì²« ë²ˆì§¸ ì»¬ëŸ¼ì„ í…ìŠ¤íŠ¸ë¡œ ê°€ì •
        text_col = [c for c in examples.keys() if c != 'label'][0]
        
    return tokenizer(
        examples[text_col], 
        truncation=True, 
        padding="max_length", 
        max_length=MAX_LEN
    )

tokenized_train = train_dataset.map(preprocess_function, batched=True)
tokenized_eval = eval_dataset.map(preprocess_function, batched=True)

# ==========================================
# 4. ëª¨ë¸ ë° í•™ìŠµ ì„¤ì • (ë©”ëª¨ë¦¬ ìµœì í™” ì ìš©)
# ==========================================
# GPU ìºì‹œ ì •ë¦¬
gc.collect()
torch.cuda.empty_cache()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"â–¶ í•™ìŠµ ì¥ì¹˜: {device}")

model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME, 
    num_labels=NUM_LABELS,
    trust_remote_code=True,
    ignore_mismatched_sizes=True # ë ˆì´ì–´ í¬ê¸°ê°€ ë‹¬ë¼ë„ ê°•ì œ ë¡œë“œ (ì¬í•™ìŠµìš©)
)
model.to(device)

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    acc = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average="macro")
    return {"accuracy": acc, "f1": f1}

# --- [ìë™ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ê³„ì‚°] ---
# OOM ë°©ì§€ë¥¼ ìœ„í•´ GPUë‹¹ ë°°ì¹˜ ì‚¬ì´ì¦ˆëŠ” 4ë¡œ ê³ ì •í•˜ê³ ,
# Gradient Accumulationìœ¼ë¡œ ëª©í‘œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ(64)ë¥¼ ë§ì¶¥ë‹ˆë‹¤.
SAFE_GPU_BATCH_SIZE = 4 

if TARGET_BATCH_SIZE < SAFE_GPU_BATCH_SIZE:
    calculated_accum_steps = 1
    real_batch_size = TARGET_BATCH_SIZE
else:
    calculated_accum_steps = TARGET_BATCH_SIZE // SAFE_GPU_BATCH_SIZE
    real_batch_size = SAFE_GPU_BATCH_SIZE

print(f"ğŸ”§ [OOM ë°©ì§€ ì„¤ì •] ëª©í‘œ ë°°ì¹˜: {TARGET_BATCH_SIZE} -> ì‹¤ì œ ë°°ì¹˜: {real_batch_size} x ëˆ„ì : {calculated_accum_steps}íšŒ")

training_args = TrainingArguments(
    output_dir=CHECKPOINT_DIR,
    learning_rate=LR,
    
    # ìˆ˜ì •ëœ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì ìš©
    per_device_train_batch_size=real_batch_size,
    per_device_eval_batch_size=real_batch_size,
    gradient_accumulation_steps=calculated_accum_steps,
    
    # ë©”ëª¨ë¦¬ ì ˆì•½ ì˜µì…˜ (í•„ìˆ˜)
    fp16=True,                   # Mixed Precision ì‚¬ìš©
    gradient_checkpointing=False, 
    num_train_epochs=EPOCHS,
    weight_decay=0.01,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    save_total_limit=2,
    seed=SEED,
    logging_dir=f"{OUTPUT_DIR}/logs",
    logging_steps=50,
    disable_tqdm=False,
    optim="adamw_torch"          # PyTorch ë„¤ì´í‹°ë¸Œ ì˜µí‹°ë§ˆì´ì € ì‚¬ìš©
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_eval,
    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),
    compute_metrics=compute_metrics,
    callbacks=[DescriptionCallback()],
)

# ==========================================
# 5. í•™ìŠµ ì‹¤í–‰ ë° ì•ˆì „ ì €ì¥
# ==========================================
print("ğŸš€ í•™ìŠµ ì‹œì‘...")

try:
    trainer.train()
except TypeError as e:
    if "filename_prefix" in str(e) or "save_vocabulary" in str(e):
        print("âš  Trainer ìë™ ì €ì¥ ì¤‘ í˜¸í™˜ì„± ì´ìŠˆ ë°œìƒ (ë¬´ì‹œí•˜ê³  ìˆ˜ë™ ì €ì¥ ì§„í–‰)")
    else:
        print(f"âŒ í•™ìŠµ ì¤‘ ì¹˜ëª…ì  ì—ëŸ¬ ë°œìƒ: {e}")
        model.save_pretrained(f"{OUTPUT_DIR}_emergency")
        raise e
except Exception as e:
    print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}")
    # ë¹„ìƒ ì €ì¥ ì‹œë„
    if not os.path.exists(f"{OUTPUT_DIR}_emergency"):
        os.makedirs(f"{OUTPUT_DIR}_emergency")
    model.save_pretrained(f"{OUTPUT_DIR}_emergency")
    raise e

print(f"ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥ ì¤‘... ({OUTPUT_DIR})")
# ëª¨ë¸ ì €ì¥
model.save_pretrained(OUTPUT_DIR)

# í† í¬ë‚˜ì´ì € ì €ì¥
try:
    tokenizer.save_pretrained(OUTPUT_DIR)
except Exception as e:
    print(f"âš  í† í¬ë‚˜ì´ì € ì €ì¥ ì‹¤íŒ¨: {e}")

print("ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")