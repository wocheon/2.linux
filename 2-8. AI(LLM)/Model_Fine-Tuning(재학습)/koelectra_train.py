import configparser
import os
import torch
import numpy as np
from datasets import load_dataset
from transformers import (
    ElectraTokenizer,
    ElectraForSequenceClassification,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding,
    TrainerCallback
)
from sklearn.metrics import accuracy_score, f1_score

# WANDB ë¹„í™œì„±í™” (í•„ìš” ì‹œ ì£¼ì„ í•´ì œ)
os.environ["WANDB_DISABLED"] = "true"

# ==========================================
# 0. ìœ í‹¸ë¦¬í‹° & ì½œë°±
# ==========================================
def get_data_format(file_path):
    ext = file_path.split('.')[-1].lower()
    if ext == 'csv': return 'csv'
    elif ext in ['json', 'jsonl']: return 'json'
    elif ext == 'parquet': return 'parquet'
    return 'csv'

class DescriptionCallback(TrainerCallback):
    """í•™ìŠµ ì§„í–‰ ìƒí™©ì„ ì§ê´€ì ìœ¼ë¡œ ë³´ì—¬ì£¼ëŠ” ì½œë°±"""
    def on_log(self, args, state, control, logs=None, **kwargs):
        if state.is_local_process_zero and logs:
            if 'loss' in logs:
                print(f"[Epoch: {logs.get('epoch', 0):.2f}] Loss: {logs['loss']:.4f}")

# ==========================================
# 1. Config ë¡œë“œ
# ==========================================
config = configparser.ConfigParser()
if not os.path.exists('config.ini'):
    raise FileNotFoundError("âŒ 'config.ini' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
config.read('config.ini')

# [Path]
MODEL_NAME_CFG = config['Path']['model_name']
TRAIN_FILE = os.path.abspath(config['Path']['train_file'])
VALID_FILE_RAW = config['Path'].get('valid_file', '').strip()
VALID_FILE = os.path.abspath(VALID_FILE_RAW) if VALID_FILE_RAW else ""
OUTPUT_DIR = config['Path']['output_dir']
CHECKPOINT_DIR = config['Path']['checkpoint_dir']

# [Hyperparameters]
NUM_LABELS = config.getint('Hyperparameters', 'num_labels')
MAX_LEN = config.getint('Hyperparameters', 'max_seq_length')
BATCH_SIZE = config.getint('Hyperparameters', 'batch_size')
LR = config.getfloat('Hyperparameters', 'learning_rate')
EPOCHS = config.getint('Hyperparameters', 'epochs')
SEED = config.getint('Hyperparameters', 'seed')
SPLIT_RATIO = config.getfloat('Hyperparameters', 'split_ratio')
USE_SUBSET = config.getboolean('Hyperparameters', 'use_subset', fallback=False)
SUBSET_SIZE = config.getint('Hyperparameters', 'subset_size', fallback=100)

print(f"â–¶ Config Loaded")
print(f"   - Train File: {TRAIN_FILE}")
print(f"   - Batch Size: {BATCH_SIZE} (âš  OOM ì£¼ì˜)")
print(f"   - Learning Rate: {LR}")

# ==========================================
# 2. ë°ì´í„°ì…‹ ë¡œë“œ ë° ë¶„í• 
# ==========================================
data_format = get_data_format(TRAIN_FILE)

if VALID_FILE and os.path.exists(VALID_FILE):
    print(f"â–¶ ê²€ì¦ íŒŒì¼ ì‚¬ìš©: {VALID_FILE}")
    dataset = load_dataset(data_format, data_files={"train": TRAIN_FILE, "validation": VALID_FILE})
    train_dataset = dataset["train"]
    eval_dataset = dataset["validation"]
else:
    print(f"â–¶ ê²€ì¦ íŒŒì¼ ì—†ìŒ -> í•™ìŠµ ë°ì´í„°ì—ì„œ {SPLIT_RATIO*100}% ìë™ ë¶„í• ")
    raw_dataset = load_dataset(data_format, data_files={"train": TRAIN_FILE})
    # ì‹œë“œ ê³ ì •í•˜ì—¬ ë¶„í• 
    split_datasets = raw_dataset["train"].train_test_split(test_size=SPLIT_RATIO, seed=SEED)
    train_dataset = split_datasets["train"]
    eval_dataset = split_datasets["test"]

# ì»¬ëŸ¼ëª… ì •ê·œí™” (ëŒ€ì†Œë¬¸ì ë¬´ì‹œí•˜ê³  'label'ë¡œ í†µì¼)
for col in train_dataset.column_names:
    if col.lower() == 'label' and col != 'label':
        train_dataset = train_dataset.rename_column(col, "label")
        eval_dataset = eval_dataset.rename_column(col, "label")

# Subset ëª¨ë“œ (í…ŒìŠ¤íŠ¸ìš©)
if USE_SUBSET:
    print(f"âš  [TEST MODE] ë°ì´í„° ì¶•ì†Œ ì‹¤í–‰ (Train: {SUBSET_SIZE})")
    train_dataset = train_dataset.select(range(min(len(train_dataset), SUBSET_SIZE)))
    eval_dataset = eval_dataset.select(range(min(len(eval_dataset), int(SUBSET_SIZE * 0.2))))

print(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: Train({len(train_dataset)}), Eval({len(eval_dataset)})")

# ==========================================
# 3. í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ
# ==========================================
print("â³ ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘ ...")

# ëª¨ë¸ ê²½ë¡œê°€ ë¡œì»¬ì— ì—†ìœ¼ë©´ HuggingFace Hubì˜ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš© (ì•ˆì „ì¥ì¹˜)
if not os.path.exists(MODEL_NAME_CFG) and "/" not in MODEL_NAME_CFG:
     # ê²½ë¡œë„ ì•„ë‹ˆê³  Hub IDë„ ì•„ë‹Œ ê²ƒ ê°™ì„ ë•Œ
     print(f"âš  ê²½ê³ : '{MODEL_NAME_CFG}' ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
     
try:
    # KoELECTRA Tokenizer ë¡œë“œ
    tokenizer = ElectraTokenizer.from_pretrained(MODEL_NAME_CFG)
except OSError:
    print(f"âš  '{MODEL_NAME_CFG}' ë¡œë“œ ì‹¤íŒ¨. 'monologg/koelectra-base-v3-discriminator'ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
    MODEL_NAME_CFG = "monologg/koelectra-base-v3-discriminator"
    tokenizer = ElectraTokenizer.from_pretrained(MODEL_NAME_CFG)

# ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_function(examples):
    # í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì°¾ê¸°
    col_candidates = ["text", "content", "document", "review", "title"] 
    text_col = next((c for c in col_candidates if c in examples), None)
    if not text_col:
        # labelì´ ì•„ë‹Œ ì²« ë²ˆì§¸ ì»¬ëŸ¼ì„ í…ìŠ¤íŠ¸ë¡œ ê°„ì£¼
        text_col = [c for c in examples.keys() if c != 'label'][0]
        
    return tokenizer(
        examples[text_col], 
        truncation=True, 
        padding="max_length", 
        max_length=MAX_LEN
    )

tokenized_train = train_dataset.map(preprocess_function, batched=True)
tokenized_eval = eval_dataset.map(preprocess_function, batched=True)

# ëª¨ë¸ ë¡œë“œ (Discriminator ëª…ì‹œ)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = ElectraForSequenceClassification.from_pretrained(
    MODEL_NAME_CFG, 
    num_labels=NUM_LABELS
)
model.to(device)

# ==========================================
# 4. í•™ìŠµ ì„¤ì • (Trainer)
# ==========================================
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    acc = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average="macro")
    return {"accuracy": acc, "f1": f1}

training_args = TrainingArguments(
    output_dir=CHECKPOINT_DIR,        # ì²´í¬í¬ì¸íŠ¸ ì €ì¥ì†Œ
    learning_rate=LR,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    num_train_epochs=EPOCHS,
    weight_decay=0.01,
    eval_strategy="epoch",            # ë§¤ epochë§ˆë‹¤ ê²€ì¦
    save_strategy="epoch",            # ë§¤ epochë§ˆë‹¤ ì €ì¥
    load_best_model_at_end=True,      # í•™ìŠµ ì¢…ë£Œ ì‹œ ê°€ì¥ ì¢‹ì•˜ë˜ ëª¨ë¸ ë¡œë“œ
    metric_for_best_model="accuracy", # ì •í™•ë„ ê¸°ì¤€
    save_total_limit=2,               # ìš©ëŸ‰ ê´€ë¦¬ë¥¼ ìœ„í•´ ìµœê·¼ 2ê°œë§Œ ì €ì¥
    seed=SEED,
    logging_steps=50,
    warmup_ratio=0.1,                 # KoELECTRA í•™ìŠµ ì•ˆì •í™”ë¥¼ ìœ„í•œ Warmup
    fp16=torch.cuda.is_available(),   # GPU ì‚¬ìš© ì‹œ FP16(Mixed Precision) ìë™ ì ìš© -> ì†ë„ í–¥ìƒ & ë©”ëª¨ë¦¬ ì ˆì•½
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_eval,
    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),
    compute_metrics=compute_metrics,
    callbacks=[DescriptionCallback()],
)

# ==========================================
# 5. í•™ìŠµ ì‹¤í–‰
# ==========================================
print("ğŸš€ í•™ìŠµ ì‹œì‘ ...")
trainer.train()

# ==========================================
# 6. ìµœì¢… ëª¨ë¸ ì €ì¥
# ==========================================
print(f"ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥ ì¤‘ ... ({OUTPUT_DIR})")
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print("ğŸ‰ í•™ìŠµ ì™„ë£Œ! ì´ì œ ì¶”ë¡ ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
