import configparser
import os
import torch
import numpy as np
import shutil
import logging
from datasets import load_dataset, disable_progress_bar
from transformers.trainer_utils import get_last_checkpoint
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    ElectraTokenizer,
    ElectraForSequenceClassification,    
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding,
    TrainerCallback,
    logging as hf_logging
)
from sklearn.metrics import accuracy_score, f1_score
import gc

# ==========================================
# [ì„¤ì •] í™˜ê²½ ë³€ìˆ˜ ë° ë¡œê¹… ì œì–´
# ==========================================
os.environ["WANDB_DISABLED"] = "true"
os.environ["TOKENIZERS_PARALLELISM"] = "false"
hf_logging.set_verbosity_warning()
disable_progress_bar()

# ==========================================
# 0. ìœ í‹¸ë¦¬í‹° & ì»¤ìŠ¤í…€ í´ë˜ìŠ¤
# ==========================================
def get_data_format(file_path):
    """íŒŒì¼ í™•ì¥ìë¥¼ ê¸°ë°˜ìœ¼ë¡œ load_datasetì˜ í¬ë§· íŒŒë¼ë¯¸í„°ë¥¼ ê²°ì •"""
    ext = file_path.split('.')[-1].lower()
    if ext == 'csv': return 'csv'
    elif ext in ['json', 'jsonl']: return 'json'
    elif ext == 'parquet': return 'parquet'
    else:
        print(f"âš  ì•Œ ìˆ˜ ì—†ëŠ” í™•ì¥ì(.{ext})ì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ 'csv'ë¡œ ì‹œë„í•©ë‹ˆë‹¤.")
        return 'csv'

def get_model_classes(model_name):
    """ëª¨ë¸ ì´ë¦„ì— ë”°ë¼ ì ì ˆí•œ Tokenizer/Model í´ë˜ìŠ¤ ë°˜í™˜"""
    model_name_lower = model_name.lower()
    if "koelectra" in model_name_lower:
        print(f"âš¡ KoELECTRA ê°ì§€: Electra ì „ìš© í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return ElectraTokenizer, ElectraForSequenceClassification
    else:
        print(f"ğŸ¤– ì¼ë°˜ ëª¨ë¸ ê°ì§€: Auto í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return AutoTokenizer, AutoModelForSequenceClassification

class DescriptionCallback(TrainerCallback):
    """
    í•™ìŠµ ë¡œê·¸ë¥¼ ì´ëª¨ì§€ì™€ í•¨ê»˜ ì¹œì ˆí•˜ê²Œ ì¶œë ¥í•˜ëŠ” ì»¤ìŠ¤í…€ ì½œë°±
    (í•™ìŠµ ì¤‘ Loss ë³€í™”ì™€ ê²€ì¦ ê²°ê³¼ë¥¼ ëª¨ë‘ ë³´ì—¬ì¤ë‹ˆë‹¤)
    """
    def on_log(self, args, state, control, logs=None, **kwargs):
        if state.is_local_process_zero and logs:
            epoch = logs.get('epoch', 0)
            
            # 1. í•™ìŠµ ì¤‘ Loss ë¡œê·¸ (Training Loss)
            if 'loss' in logs and 'eval_loss' not in logs:
                loss_val = logs['loss']
                if loss_val > 0.8: comment = "ğŸ˜° ì•„ì§ í—¤ë§¤ëŠ” ì¤‘..."
                elif loss_val > 0.5: comment = "ğŸ¤” ê°ì„ ì¡ëŠ” ì¤‘..."
                elif loss_val > 0.3: comment = "ğŸ™‚ í•™ìŠµì´ ì˜ ë˜ê³  ìˆì–´ìš”!"
                else: comment = "ğŸš€ ì™„ë²½í•´ìš”!"
                
                print(f"[Epoch {epoch:.2f}] Loss: {loss_val:.4f} -> {comment}")
                if 'learning_rate' in logs:
                    print(f"   â””â”€ LR: {logs['learning_rate']:.2e}")

            # 2. ê²€ì¦ ê²°ê³¼ ë¡œê·¸ (Validation Metrics)
            if 'eval_accuracy' in logs:
                acc = logs['eval_accuracy']
                f1 = logs.get('eval_f1', 0)
                loss = logs.get('eval_loss', 0)
                
                # ê²€ì¦ ì ìˆ˜ì— ë”°ë¥¸ ì½”ë©˜íŠ¸
                if acc < 0.6: score_comment = "ğŸš§ ì•„ì§ ë¶€ì¡±í•´ìš”"
                elif acc < 0.8: score_comment = "âœ¨ ì“¸ë§Œí•´ì§€ê³  ìˆì–´ìš”"
                else: score_comment = "ğŸ† í›Œë¥­í•©ë‹ˆë‹¤!"
                
                print(f"\nâœ¨ [Epoch {epoch:.2f} ê²€ì¦ ì™„ë£Œ] {score_comment}")
                print(f"   â””â”€ Acc: {acc:.4f} | F1: {f1:.4f} | Loss: {loss:.4f}\n")

class SilentEvalTrainer(Trainer):
    """ê²€ì¦ ì‹œ TQDM ë°”ë¥¼ ë„ëŠ” ì»¤ìŠ¤í…€ íŠ¸ë ˆì´ë„ˆ"""
    def prediction_loop(self, dataloader, description, prediction_loss_only=None, ignore_keys=None, metric_key_prefix="eval"):
        original_disable_tqdm = self.args.disable_tqdm
        self.args.disable_tqdm = True
        try:
            return super().prediction_loop(dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)
        finally:
            self.args.disable_tqdm = original_disable_tqdm
        
        return output

# ==========================================
# 1. Config ë¡œë“œ
# ==========================================
config = configparser.ConfigParser()
config_path = 'config.ini'

if not os.path.exists(config_path):
    print(f"âš  ê²½ê³ : '{config_path}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ì„¤ì •ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    config['Path'] = {
        'model_name': 'klue/roberta-base',
        'train_file': 'dataset.csv',
        'output_dir': './output',
        'checkpoint_dir': './checkpoints'
    }
    config['Hyperparameters'] = {
        'num_labels': '3',
        'max_seq_length': '128',
        'batch_size': '64',
        'learning_rate': '2e-5',
        'epochs': '3',
        'seed': '42',
        'split_ratio': '0.2',
        'min_confidence': '0.8'
    }
else:
    config.read(config_path)

# [Path] ì„¹ì…˜ ë¡œë“œ
MODEL_NAME = config['Path']['model_name']
TRAIN_FILE = os.path.abspath(config['Path']['train_file'])
VALID_FILE_RAW = config['Path'].get('valid_file', '').strip()
VALID_FILE = os.path.abspath(VALID_FILE_RAW) if VALID_FILE_RAW else ""
OUTPUT_DIR = config['Path']['output_dir']
CHECKPOINT_DIR = config['Path']['checkpoint_dir']

# [Hyperparameters] ì„¹ì…˜ ë¡œë“œ
params = config['Hyperparameters']
NUM_LABELS = int(params.get('num_labels', 3))
MAX_LEN = int(params.get('max_seq_length', 128))
TARGET_BATCH_SIZE = int(params.get('batch_size', 64))
LR = float(params.get('learning_rate', 2e-5))
EPOCHS = int(params.get('epochs', 3))
SEED = int(params.get('seed', 42))
SPLIT_RATIO = float(params.get('split_ratio', 0.2))
MIN_CONFIDENCE = float(params.get('min_confidence', 0.8))

# [Subset] ì˜µì…˜
USE_SUBSET = config.getboolean('Hyperparameters', 'use_subset', fallback=False)
SUBSET_SIZE = config.getint('Hyperparameters', 'subset_size', fallback=100)

print(f"â–¶ ëª¨ë¸: {MODEL_NAME}")
print(f"â–¶ í•™ìŠµ íŒŒì¼: {TRAIN_FILE}")

# ì‚¬ìš©í•  í´ë˜ìŠ¤ ê²°ì •
TokenizerClass, ModelClass = get_model_classes(MODEL_NAME)

# ==========================================
# 2. ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬
# ==========================================
data_format = get_data_format(TRAIN_FILE)
raw_dataset = load_dataset(data_format, data_files={"train": TRAIN_FILE})['train']

# [Step A] ë°ì´í„° í•„í„°ë§
def filter_and_format(example):
    # 1. Confidence ì²´í¬
    if 'confidence' in example and example['confidence'] is not None:
        try:
            if float(example['confidence']) < MIN_CONFIDENCE:
                return False
        except:
            pass
    # 2. Label ìœ íš¨ì„± ì²´í¬
    if example.get('label_sentiment') not in [0, 1, 2]:
        return False
    return True

print(f"ğŸ“‰ í’ˆì§ˆ í•„í„°ë§ ì „: {len(raw_dataset)}ê±´")
filtered_dataset = raw_dataset.filter(filter_and_format)
print(f"ğŸ“ˆ í’ˆì§ˆ í•„í„°ë§ í›„: {len(filtered_dataset)}ê±´ (ê¸°ì¤€: conf >= {MIN_CONFIDENCE})")

# [Step B] ì…ë ¥ í…ìŠ¤íŠ¸ ì¡°í•© (DeBERTa ìì—°ì–´ í¬ë§· ì ìš©)
def combine_text(example):
    tgt = example.get('target', 'ì‹œì¥')
    title = example.get('title', '')
    
    # [í•µì‹¬ ë³€ê²½] "[Target] Title" -> "Target ê´€ë ¨ ë‰´ìŠ¤: Title"
    combined_text = f"{tgt} ê´€ë ¨ ë‰´ìŠ¤: {title}"
    
    return {
        "text": combined_text, 
        "label": int(example['label_sentiment']) 
    }

processed_dataset = filtered_dataset.map(combine_text, remove_columns=filtered_dataset.column_names)

# í•™ìŠµ/ê²€ì¦ ë¶„í• 
if VALID_FILE and os.path.exists(VALID_FILE):
    raw_eval = load_dataset(data_format, data_files={"validation": VALID_FILE})['validation']
    eval_dataset = raw_eval.filter(filter_and_format).map(combine_text, remove_columns=raw_eval.column_names)
    train_dataset = processed_dataset
else:
    if len(processed_dataset) < 10:
        raise ValueError("âŒ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤.")
    split_datasets = processed_dataset.train_test_split(test_size=SPLIT_RATIO, seed=SEED)
    train_dataset = split_datasets["train"]
    eval_dataset = split_datasets["test"]

if USE_SUBSET:
    print(f"\nâš  [í…ŒìŠ¤íŠ¸ ëª¨ë“œ] ë°ì´í„° ì¶•ì†Œ ì‹¤í–‰")
    if len(train_dataset) > SUBSET_SIZE:
        train_dataset = train_dataset.select(range(SUBSET_SIZE))
    eval_dataset = eval_dataset.select(range(min(len(eval_dataset), int(SUBSET_SIZE * 0.2))))

print(f"âœ… ìµœì¢… ë°ì´í„°: í•™ìŠµ({len(train_dataset)}) / ê²€ì¦({len(eval_dataset)})")
print(f"ğŸ‘€ ì…ë ¥ ë³€í™˜ ì˜ˆì‹œ: '{train_dataset[0]['text']}' -> ë¼ë²¨: {train_dataset[0]['label']}")

# ==========================================
# 3. í† í¬ë‚˜ì´ì € ë¡œë“œ
# ==========================================
print("â³ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")

tokenizer = TokenizerClass.from_pretrained(
    MODEL_NAME, 
    trust_remote_code=True
)

# [Monkey Patch] save_vocabulary í˜¸í™˜ì„± ë³´ì¥
if not hasattr(tokenizer, "_original_save_vocabulary"):
    if hasattr(tokenizer, "save_vocabulary"):
        tokenizer._original_save_vocabulary = tokenizer.save_vocabulary

def patched_save_vocabulary(save_directory, filename_prefix=None):
    if hasattr(tokenizer, "_original_save_vocabulary"):
        return tokenizer._original_save_vocabulary(save_directory)
    else:
        return ()
tokenizer.save_vocabulary = patched_save_vocabulary

def preprocess_function(examples):
    return tokenizer(
        examples["text"], 
        truncation=True, 
        padding="max_length", 
        max_length=MAX_LEN
    )

tokenized_train = train_dataset.map(preprocess_function, batched=True)
tokenized_eval = eval_dataset.map(preprocess_function, batched=True)

# ==========================================
# 4. ëª¨ë¸ ë° í•™ìŠµ ì„¤ì •
# ==========================================
gc.collect()
torch.cuda.empty_cache()
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"â–¶ í•™ìŠµ ì¥ì¹˜: {device}")

model = ModelClass.from_pretrained(
    MODEL_NAME, 
    num_labels=NUM_LABELS,
    trust_remote_code=True,
    ignore_mismatched_sizes=True
)
model.to(device)

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    acc = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average="macro")
    return {"accuracy": acc, "f1": f1}

# ë°°ì¹˜ ì‚¬ì´ì¦ˆ ê³„ì‚°
SAFE_GPU_BATCH_SIZE = 4 
if TARGET_BATCH_SIZE < SAFE_GPU_BATCH_SIZE:
    calculated_accum_steps = 1
    real_batch_size = TARGET_BATCH_SIZE
else:
    calculated_accum_steps = TARGET_BATCH_SIZE // SAFE_GPU_BATCH_SIZE
    real_batch_size = SAFE_GPU_BATCH_SIZE

print(f"ğŸ”§ [ì„¤ì •] ëª©í‘œ ë°°ì¹˜: {TARGET_BATCH_SIZE} -> ì‹¤ì œ: {real_batch_size} (ëˆ„ì  {calculated_accum_steps}íšŒ)")

training_args = TrainingArguments(
    output_dir=CHECKPOINT_DIR,
    learning_rate=LR,
    per_device_train_batch_size=real_batch_size,
    per_device_eval_batch_size=real_batch_size,
    gradient_accumulation_steps=calculated_accum_steps,
    fp16=True,
    num_train_epochs=EPOCHS,
    weight_decay=0.01,
    eval_strategy="epoch",
    save_strategy="epoch",    
    load_best_model_at_end=True,
    save_total_limit=2,
    seed=SEED,
    logging_dir=f"{OUTPUT_DIR}/logs",
    logging_steps=100,
    optim="adamw_torch",
    
    # [ë¡œê·¸ ì„¤ì •]
    disable_tqdm=False,  # í•™ìŠµ ì§„í–‰ë°”ëŠ” ìœ ì§€ (ê²€ì¦ì€ SilentEvalTrainerê°€ ì œì–´)
    log_level="error",
    report_to=["none"]
)

# [í•µì‹¬] SilentEvalTrainer ì‚¬ìš©
trainer = SilentEvalTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_eval,
    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),
    compute_metrics=compute_metrics,
    callbacks=[DescriptionCallback()],
)

# ==========================================
# 5. í•™ìŠµ ì‹¤í–‰ ë° ì €ì¥
# ==========================================
print("ğŸš€ í•™ìŠµ ì‹œì‘...")
last_checkpoint = get_last_checkpoint(CHECKPOINT_DIR) if os.path.isdir(CHECKPOINT_DIR) else None

trainer.train(resume_from_checkpoint=last_checkpoint)

print(f"ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥: {OUTPUT_DIR}")
model.save_pretrained(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)
print("ğŸ‰ ì™„ë£Œ!")
