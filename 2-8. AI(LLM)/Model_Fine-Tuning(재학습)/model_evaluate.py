import os
import numpy as np
import evaluate
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    ElectraTokenizer,
    ElectraForSequenceClassification,
    Trainer,
    TrainingArguments
)

# ==========================================================
# 1. ì„¤ì •
# ==========================================================
os.environ["WANDB_DISABLED"] = "true"

# í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ
#MODEL_PATH = "./models/fine_tunned_debert"
#MODEL_PATH = "./models/fine_tunned_kobert"
MODEL_PATH = "./models/fine_tunned_koelectra"

# ê²€ì¦ìš© ë°ì´í„°ì…‹ ê²½ë¡œ (ë‹¨ì¼ íŒŒì¼)
EVAL_DATA_PATH = "./dataset_dir/evaluate_dataset.csv"

# ==========================================================
# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
# ==========================================================

print(f"â³ ëª¨ë¸ ë¡œë“œ ì¤‘: {MODEL_PATH}")

# ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ í´ë˜ìŠ¤ ìë™ ì„ íƒ ë¡œì§
def get_model_classes(model_path):
    path_lower = model_path.lower()
    
    # 1. KoELECTRAì¸ ê²½ìš°
    if "koelectra" in path_lower:
        print(f"âš¡ KoELECTRA ê°ì§€: Electra ì „ìš© í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ({model_path}), Class : ElectraTokenizer, ElectraForSequenceClassification")
        return ElectraTokenizer, ElectraForSequenceClassification
        
    # 2. ê·¸ ì™¸ (DeBERTa, RoBERTa, KoBERT ë“±) -> Auto í´ë˜ìŠ¤ ì‚¬ìš©
    else:
        print(f"ğŸ¤– ì¼ë°˜ ëª¨ë¸ ê°ì§€: Auto í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ({model_path}), Class : AutoTokenizer, AutoModelForSequenceClassification")
        return AutoTokenizer, AutoModelForSequenceClassification

# ì‚¬ìš©í•  í´ë˜ìŠ¤ ê²°ì •
TokenizerClass, ModelClass = get_model_classes(MODEL_PATH)

try:
    tokenizer = TokenizerClass.from_pretrained(MODEL_PATH, trust_remote_code=True)
    model = ModelClass.from_pretrained(MODEL_PATH)
except Exception as e:
    print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    exit(1)

# ==========================================================
# 3. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (í•µì‹¬ ìˆ˜ì • êµ¬ê°„)
# ==========================================================
print(f"ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘: {EVAL_DATA_PATH}")
dataset = load_dataset("csv", data_files=EVAL_DATA_PATH, split="train")

print(f"âœ… ì›ë³¸ ë°ì´í„° ê°œìˆ˜: {len(dataset)}")

# ----------------------------------------------------------
# [ì¤‘ìš”] í•™ìŠµ ë•Œì™€ ë™ì¼í•œ ì…ë ¥ í¬ë§· ë§Œë“¤ê¸°
# í˜•ì‹: [Target] Title
# ----------------------------------------------------------
def format_input(example):
    # None ê°’ ë°©ì–´ ì²˜ë¦¬
    tgt = example.get('target')
    if tgt is None: tgt = 'ì‹œì¥'
    
    title = example.get('title')
    if title is None: title = ''
    
    # í•™ìŠµ ì½”ë“œì˜ combine_text í•¨ìˆ˜ì™€ ë™ì¼í•œ ë¡œì§ ì ìš©
    combined_text = f"[{tgt}] {title}"
    
    # ì»¬ëŸ¼ ë§¤í•‘ (label_sentiment -> label)
    try:
        label = int(example['label_sentiment'])
    except (ValueError, TypeError):
        label = 0 # ì—ëŸ¬ ì‹œ ì¤‘ë¦½ ì²˜ë¦¬

    return {
        "text": combined_text, 
        "label": label
    }

print("âš™ï¸ ì…ë ¥ í¬ë§· ë³€í™˜ ì¤‘ ([Target] Title)...")
formatted_dataset = dataset.map(format_input, remove_columns=dataset.column_names)

# ----------------------------------------------------------
# í† í°í™” (Tokenization)
# ----------------------------------------------------------
def preprocess_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        max_length=128,
        padding="max_length"
    )

print("âš™ï¸ í† í°í™” ì§„í–‰ ì¤‘...")
tokenized_dataset = formatted_dataset.map(preprocess_function, batched=True)

print(f"âœ… ìµœì¢… í‰ê°€ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(tokenized_dataset)}ê°œ")
# print(f"ğŸ‘€ ì…ë ¥ ì˜ˆì‹œ: {tokenized_dataset[0]['text']} -> {tokenized_dataset[0]['label']}")

# ==========================================================
# 4. í‰ê°€ ì§€í‘œ ì •ì˜
# ==========================================================
accuracy_metric = evaluate.load("accuracy")
f1_metric = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)

    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)
    # average="macro": í´ë˜ìŠ¤ ë¶ˆê· í˜• ê³ ë ¤ (0,1,2 ê³¨ê³ ë£¨ ì˜ ë§ì¶”ëŠ”ì§€)
    f1 = f1_metric.compute(predictions=predictions, references=labels, average="macro")

    return {
        "accuracy": accuracy["accuracy"],
        "f1": f1["f1"]
    }

# ==========================================================
# 5. í‰ê°€ ì‹¤í–‰
# ==========================================================
training_args = TrainingArguments(
    output_dir="./temp_eval_results",
    report_to="none",
    per_device_eval_batch_size=64,  # í‰ê°€ ì†ë„ë¥¼ ìœ„í•´ í¬ê²Œ ì„¤ì •
    dataloader_num_workers=4        # ë°ì´í„° ë¡œë”© ì†ë„ í–¥ìƒ
)

trainer = Trainer(
    model=model,
    args=training_args,
    eval_dataset=tokenized_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

print("ğŸš€ í‰ê°€ ì‹œì‘...")
metrics = trainer.evaluate()

print("\nğŸ“Š ìµœì¢… í‰ê°€ ì„±ì í‘œ:")
print("=" * 30)
for key, value in metrics.items():
    if "eval_" in key:
        key = key.replace("eval_", "")
    print(f"{key}: {value:.4f}")
print("=" * 30)
