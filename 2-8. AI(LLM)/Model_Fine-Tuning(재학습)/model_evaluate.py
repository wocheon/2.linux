import os
import glob
import numpy as np
import evaluate
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments

# ==========================================================
# 1. ì„¤ì • (ê²½ë¡œ ë° ì»¬ëŸ¼ëª… í™•ì¸ í•„ìˆ˜)
# ==========================================================
# WandB ë„ê¸° (ì—ëŸ¬ ë°©ì§€)
os.environ["WANDB_DISABLED"] = "true"

# ëª¨ë¸ ê²½ë¡œ
MODEL_PATH = "./models/fine_tunned_debert"

csv_files =  "./dataset_dir/balanced_sentiment_eval_500_utf8sig.csv"

# CSV ë‚´ ì»¬ëŸ¼ ì´ë¦„ (ì‹¤ì œ íŒŒì¼ê³¼ ì¼ì¹˜í•´ì•¼ í•¨!)
TEXT_COLUMN = "text"
LABEL_COLUMN = "label"

# ==========================================================
# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
# ==========================================================
print(f"â³ ëª¨ë¸ ë¡œë“œ ì¤‘: {MODEL_PATH}")
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)

# ==========================================================
# 3. ë°ì´í„° ë¡œë“œ (ì—¬ëŸ¬ CSV í•©ì¹˜ê¸°)
# ==========================================================
# í´ë” ë‚´ ëª¨ë“  .csv íŒŒì¼ ì°¾ê¸°

combined_dataset = load_dataset("csv", data_files=csv_files, split="train")

print(f"âœ… ì´ ë°ì´í„° ê°œìˆ˜: {len(combined_dataset)}")

# ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_function(examples):
    # Noneì´ë‚˜ ë¹„ë¬¸ìì—´ ë°ì´í„° ë°©ì–´ ì½”ë“œ
    texts = [str(x) if x is not None else "" for x in examples[TEXT_COLUMN]]
    return tokenizer(
        texts,
        truncation=True,
        max_length=128,
        padding="max_length"
    )

print("âš™ï¸ ë°ì´í„° ì „ì²˜ë¦¬(í† í°í™”) ì¤‘...")
tokenized_dataset = combined_dataset.map(preprocess_function, batched=True)

# ì—¬ê¸°ì„œëŠ” 'ì „ì²´ ë°ì´í„°ë¥¼ í‰ê°€ìš©'ìœ¼ë¡œ ì“´ë‹¤ê³  ê°€ì • (ì´ë¯¸ í•™ìŠµì´ ëë‚œ ëª¨ë¸ì´ë¯€ë¡œ)
# ë§Œì•½ ì—¬ê¸°ì„œë„ ì¼ë¶€ë§Œ ë½‘ê³  ì‹¶ë‹¤ë©´ .select() ë“±ì„ ì‚¬ìš©
test_dataset = tokenized_dataset

print(f"âœ… ìµœì¢… í‰ê°€ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(test_dataset)}ê°œ")

# ==========================================================
# 4. í‰ê°€ ì§€í‘œ ì •ì˜
# ==========================================================
accuracy_metric = evaluate.load("accuracy")
f1_metric = evaluate.load("f1")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)

    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)
    # average="macro": í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ í‰ê·  (ë¶ˆê· í˜• ë°ì´í„°ì— ì¢‹ìŒ)
    f1 = f1_metric.compute(predictions=predictions, references=labels, average="macro")

    return {
        "accuracy": accuracy["accuracy"],
        "f1": f1["f1"]
    }

# ==========================================================
# 5. í‰ê°€ ì‹¤í–‰
# ==========================================================
training_args = TrainingArguments(
    output_dir="./temp_eval_results",
    report_to="none",
    per_device_eval_batch_size=64  # ì†ë„ë¥¼ ìœ„í•´ ë°°ì¹˜ í¬ê²Œ
)

trainer = Trainer(
    model=model,
    args=training_args,
    eval_dataset=test_dataset,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

print("ğŸš€ í‰ê°€ ì‹œì‘...")
metrics = trainer.evaluate()

print("\nğŸ“Š ìµœì¢… í‰ê°€ ì„±ì í‘œ:")
print("=" * 30)
for key, value in metrics.items():
    print(f"{key}: {value:.4f}")
print("=" * 30)
