import configparser
import os
import torch
import numpy as np
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding,
    TrainerCallback
)
from sklearn.metrics import accuracy_score, f1_score

# ==========================================
# 0. ìœ í‹¸ë¦¬í‹° & ì½œë°± í´ë˜ìŠ¤
# ==========================================
def get_data_format(file_path):
    """íŒŒì¼ í™•ì¥ìë¥¼ ê¸°ë°˜ìœ¼ë¡œ load_datasetì˜ í¬ë§· íŒŒë¼ë¯¸í„°ë¥¼ ê²°ì •"""
    ext = file_path.split('.')[-1].lower()
    if ext == 'csv': return 'csv'
    elif ext in ['json', 'jsonl']: return 'json'
    elif ext == 'parquet': return 'parquet'
    else:
        print(f"âš  ì•Œ ìˆ˜ ì—†ëŠ” í™•ì¥ì(.{ext})ì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ 'csv'ë¡œ ì‹œë„í•©ë‹ˆë‹¤.")
        return 'csv'

class DescriptionCallback(TrainerCallback):
    """í•™ìŠµ ë¡œê·¸ë¥¼ ë” ì¹œì ˆí•˜ê²Œ ì¶œë ¥í•˜ëŠ” ì»¤ìŠ¤í…€ ì½œë°±"""
    def on_log(self, args, state, control, logs=None, **kwargs):
        if state.is_local_process_zero and logs:
            epoch_info = f"[Epoch: {logs.get('epoch', 0):.2f}]"
            if 'loss' in logs:
                loss_val = logs['loss']
                if loss_val > 0.8: comment = "ğŸ˜° ì•„ì§ í—¤ë§¤ëŠ” ì¤‘..."
                elif loss_val > 0.5: comment = "ğŸ¤” ê°ì„ ì¡ëŠ” ì¤‘..."
                elif loss_val > 0.3: comment = "ğŸ™‚ í•™ìŠµì´ ì˜ ë˜ê³  ìˆì–´ìš”!"
                else: comment = "ğŸš€ ì™„ë²½í•´ìš”!"
                print(f"{epoch_info} Loss: {loss_val:.4f} -> {comment}")
            if 'learning_rate' in logs:
                print(f"   â””â”€ LR: {logs['learning_rate']:.2e}")

# ==========================================
# 1. Config ë¡œë“œ
# ==========================================
config = configparser.ConfigParser()
if not os.path.exists('config.ini'):
    raise FileNotFoundError("âŒ 'config.ini' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
config.read('config.ini')

# [Path]
MODEL_NAME = config['Path']['model_name']
TRAIN_FILE = os.path.abspath(config['Path']['train_file'])
VALID_FILE_RAW = config['Path'].get('valid_file', '').strip()
VALID_FILE = os.path.abspath(VALID_FILE_RAW) if VALID_FILE_RAW else ""
OUTPUT_DIR = config['Path']['output_dir']
CHECKPOINT_DIR = config['Path']['checkpoint_dir']

# [Hyperparameters]
NUM_LABELS = config.getint('Hyperparameters', 'num_labels')
MAX_LEN = config.getint('Hyperparameters', 'max_seq_length')
BATCH_SIZE = config.getint('Hyperparameters', 'batch_size')
LR = config.getfloat('Hyperparameters', 'learning_rate')
EPOCHS = config.getint('Hyperparameters', 'epochs')
SEED = config.getint('Hyperparameters', 'seed')
SPLIT_RATIO = config.getfloat('Hyperparameters', 'split_ratio')

# [Subset]
USE_SUBSET = config.getboolean('Hyperparameters', 'use_subset', fallback=False)
SUBSET_SIZE = config.getint('Hyperparameters', 'subset_size', fallback=100)

print(f"â–¶ ëª¨ë¸: {MODEL_NAME}")
print(f"â–¶ í•™ìŠµ íŒŒì¼: {TRAIN_FILE}")

# ==========================================
# 2. ë°ì´í„°ì…‹ ë¡œë“œ
# ==========================================
data_format = get_data_format(TRAIN_FILE)
print(f"â–¶ ê°ì§€ëœ í¬ë§·: {data_format}")

if VALID_FILE and os.path.exists(VALID_FILE):
    print(f"â–¶ ê²€ì¦ íŒŒì¼: {VALID_FILE}")
    dataset = load_dataset(data_format, data_files={"train": TRAIN_FILE, "validation": VALID_FILE})
    train_dataset = dataset["train"]
    eval_dataset = dataset["validation"]
else:
    print(f"â–¶ ê²€ì¦ íŒŒì¼ ì—†ìŒ (í•™ìŠµ ë°ì´í„°ì—ì„œ {SPLIT_RATIO*100}% ìë™ ë¶„í• )")
    raw_dataset = load_dataset(data_format, data_files={"train": TRAIN_FILE})
    if len(raw_dataset["train"]) < 10:
        raise ValueError("âŒ ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤. ìµœì†Œ 10ê°œ ì´ìƒ í•„ìš”í•©ë‹ˆë‹¤.")
    split_datasets = raw_dataset["train"].train_test_split(test_size=SPLIT_RATIO, seed=SEED)
    train_dataset = split_datasets["train"]
    eval_dataset = split_datasets["test"]

# ì»¬ëŸ¼ëª… í†µì¼ (Label, LABEL -> label)
for col in train_dataset.column_names:
    if col.lower() == 'label' and col != 'label':
        train_dataset = train_dataset.rename_column(col, "label")
        eval_dataset = eval_dataset.rename_column(col, "label")

if USE_SUBSET:
    print(f"\nâš  [í…ŒìŠ¤íŠ¸ ëª¨ë“œ] ë°ì´í„° ì¶•ì†Œ ì‹¤í–‰")
    if len(train_dataset) > SUBSET_SIZE:
        train_dataset = train_dataset.select(range(SUBSET_SIZE))
    eval_dataset = eval_dataset.select(range(min(len(eval_dataset), int(SUBSET_SIZE * 0.2))))

print(f"âœ… ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: í•™ìŠµ({len(train_dataset)}) / ê²€ì¦({len(eval_dataset)})")

# ==========================================
# 3. í† í¬ë‚˜ì´ì € ë¡œë“œ ë° íŒ¨ì¹˜ (ì¤‘ìš”!)
# ==========================================
print("â³ í† í¬ë‚˜ì´ì € ë¡œë“œ ì¤‘...")
try:
    # trust_remote_code=True ì¶”ê°€ë¡œ ë³´ì•ˆ ê²½ê³  ìë™ ìŠ¹ì¸
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
except Exception as e:
    print(f"âš  í† í¬ë‚˜ì´ì € ë¡œë“œ ì‹¤íŒ¨. SentencePiece ë¬¸ì œì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì—ëŸ¬: {e}")
    raise e

# [Monkey Patch] save_vocabulary í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°
# KoBERT êµ¬ë²„ì „ í† í¬ë‚˜ì´ì €ê°€ filename_prefix ì¸ìë¥¼ ëª» ë°›ì•„ì„œ í„°ì§€ëŠ” ë¬¸ì œ í•´ê²°
if not hasattr(tokenizer, "_original_save_vocabulary"):
    # ì›ë˜ ë©”ì„œë“œê°€ ìˆë‹¤ë©´ ë°±ì—…í•´ë‘ê³  (ì—†ìœ¼ë©´ íŒ¨ìŠ¤)
    if hasattr(tokenizer, "save_vocabulary"):
        tokenizer._original_save_vocabulary = tokenizer.save_vocabulary

def patched_save_vocabulary(save_directory, filename_prefix=None):
    # filename_prefix ì¸ìê°€ ë“¤ì–´ì™€ë„ ë¬´ì‹œí•˜ê³ , ì›ë˜ ì €ì¥ ë¡œì§ ì‹¤í–‰
    if hasattr(tokenizer, "_original_save_vocabulary"):
        return tokenizer._original_save_vocabulary(save_directory)
    else:
        # ì›ë˜ ë©”ì„œë“œë„ ì—†ìœ¼ë©´ ê·¸ëƒ¥ ë¹ˆ íŠœí”Œ ë°˜í™˜ (ì—ëŸ¬ ë°©ì§€)
        return ()

# ë©”ì„œë“œ ë®ì–´ì“°ê¸°
tokenizer.save_vocabulary = patched_save_vocabulary
print("ğŸ”§ í† í¬ë‚˜ì´ì € í˜¸í™˜ì„± íŒ¨ì¹˜ ì ìš© ì™„ë£Œ")

# ì „ì²˜ë¦¬ í•¨ìˆ˜
def preprocess_function(examples):
    col_candidates = ["text", "Text", "review", "content", "document", "Review_Text"]
    text_col = next((c for c in col_candidates if c in examples), None)
    if not text_col:
        text_col = [c for c in examples.keys() if c != 'label'][0]
        
    return tokenizer(
        examples[text_col], 
        truncation=True, 
        padding="max_length", 
        max_length=MAX_LEN
    )

tokenized_train = train_dataset.map(preprocess_function, batched=True)
tokenized_eval = eval_dataset.map(preprocess_function, batched=True)

# ==========================================
# 4. ëª¨ë¸ ë° í•™ìŠµ ì„¤ì •
# ==========================================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"â–¶ í•™ìŠµ ì¥ì¹˜: {device}")

# trust_remote_code=True ì¶”ê°€ë¡œ ë³´ì•ˆ ê²½ê³  ìë™ ìŠ¹ì¸
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME, 
    num_labels=NUM_LABELS,
    trust_remote_code=True
)
model.to(device)

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    acc = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average="macro")
    return {"accuracy": acc, "f1": f1}

training_args = TrainingArguments(
    output_dir=CHECKPOINT_DIR,
    learning_rate=LR,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    num_train_epochs=EPOCHS,
    weight_decay=0.01,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    save_total_limit=2,
    seed=SEED,
    logging_dir=f"{OUTPUT_DIR}/logs",
    logging_steps=10,
    disable_tqdm=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_eval,
    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),
    compute_metrics=compute_metrics,
    callbacks=[DescriptionCallback()],
)

# ==========================================
# 5. í•™ìŠµ ì‹¤í–‰ ë° ì•ˆì „ ì €ì¥
# ==========================================
print("ğŸš€ í•™ìŠµ ì‹œì‘...")

try:
    trainer.train()
except TypeError as e:
    # filename_prefix ì—ëŸ¬ ë“±ì€ ë¬´ì‹œí•˜ê³  ì €ì¥ ë‹¨ê³„ë¡œ ì§„í–‰
    if "filename_prefix" in str(e) or "save_vocabulary" in str(e):
        print("âš  Trainer ìë™ ì €ì¥ ì¤‘ í˜¸í™˜ì„± ì´ìŠˆ ë°œìƒ (ë¬´ì‹œí•˜ê³  ìˆ˜ë™ ì €ì¥ ì§„í–‰)")
    else:
        # ì§„ì§œ ì‹¬ê°í•œ ì—ëŸ¬ëŠ” ë‹¤ì‹œ ë°œìƒì‹œí‚´
        print(f"âŒ í•™ìŠµ ì¤‘ ì¹˜ëª…ì  ì—ëŸ¬ ë°œìƒ: {e}")
        # ê·¸ë˜ë„ ëª¨ë¸ì€ ì‚´ë ¤ë³¸ë‹¤
        model.save_pretrained(f"{OUTPUT_DIR}_emergency")
        raise e
except Exception as e:
    print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}")
    model.save_pretrained(f"{OUTPUT_DIR}_emergency")
    raise e

print(f"ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥ ì¤‘... ({OUTPUT_DIR})")
# ëª¨ë¸ ì €ì¥
model.save_pretrained(OUTPUT_DIR)

# í† í¬ë‚˜ì´ì € ì €ì¥ (ì—ëŸ¬ ë°œìƒ ì‹œ ê±´ë„ˆëœ€)
try:
    tokenizer.save_pretrained(OUTPUT_DIR)
except Exception as e:
    print(f"âš  í† í¬ë‚˜ì´ì € ì €ì¥ ì‹¤íŒ¨ (ëª¨ë¸ì€ ì €ì¥ë¨): {e}")

print("ğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! (app.pyì—ì„œ ì‚¬ìš© ê°€ëŠ¥)")

