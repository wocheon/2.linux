
# AI 모델 학습 관련 내용 정리

## 모델 학습 상세 

### 모델의 생애주기 


| 시점                          | 누가?                | 무엇을?                    | 학습 방식                     | 상태                        |
| --------------------------- | ------------------ | ----------------------- | ------------------------- | ------------------------- |
| 1. 탄생(Pre-training)         | OpenAI / Meta(제조사) | GPU 1만 개로 인터넷 전체를 읽힘.   | 자기 지도 학습(Self-supervised) | Base Model(말은 잘하는데 멍청함)   |
| 2. 교육(Fine-tuning)          | OpenAI / Meta(제조사) | Q&A 데이터로 지시 따르기를 가르침.   | 지도 학습(SFT, RLHF)          | Instruct Model(말 잘 듣는 비서) |
| 3. 배포(Release)              | Hugging Face(공개)   | 우리 같은 개발자가 다운로드 받음.     | -                         | Llama-3-Instruct(완성품)     |
| 4. 활용(Inference)            | 나 (개발자)            | 내 데이터를 넣어서 결과를 뽑음.      | -                         | 학습 안 함(그냥 쓰기만 함)          |
| (옵션) 5. 재교육(My Fine-tuning) | 나 (개발자)            | 우리 회사 데이터로 '추가 교육'을 시킴. | 지도 학습(SFT)                | Custom Model(우리 회사 전용 AI) |


- 자기 지도 학습은 '제조사의 영역'
    - 개인이 하기엔 너무 고가 (전기세만 수십억 나옴)
    - 사용자는 주로 자기 지도 학습을 끝내놓은 **Pre-trained Model(사전 학습된 모델)**을 가져다 사용

- '지도 학습'은 남이 만든 똑똑한 모델(Base/Instruct)을 가져와서, 우리 데이터로 살짝만 더 가르치는 것(Fine-tuning)
    -  일반적인 AI 개발자의 업무



### 유형별 모델 학습 분류

| 유형 (Type)                    | 핵심 정의 (One-liner)                                      | 데이터 형태 (Input)                | 대표 알고리즘/모델                                                                             | 주요 용도 (Use Case)                                                       | 엔지니어링 노트 (Note)                                                          |
| ---------------------------- | ------------------------------------------------------ | ----------------------------- | -------------------------------------------------------------------------------------- | ---------------------------------------------------------------------- | ------------------------------------------------------------------------ |
| 1. 지도 학습(Supervised)         | "정답지 보고 외우기"입력과 정답 쌍을 1:1로 매핑하여 학습함.                   | 문제($X$) + 정답($Y$)(이미지, "고양이") | -  Classification (SVM, 로지스틱)-  Regression (선형, 랜덤포레스트)-  LLM SFT (Instruction Tuning) | -  스팸 분류, 암 진단-  집값/주가 예측-  LLM 챗봇화 (지시 이행)                            | -  가장 성능이 좋지만 비쌈.-  현업 AI 프로젝트의 90%는 결국 이걸 잘하는 게 목표임.                    |
| 2. 비지도 학습(Unsupervised)      | "끼리끼리 묶기"정답 없이 데이터의 숨은 구조나 그룹을 찾아냄.                    | 문제($X$) Only(고객 로그 1TB)       | -  Clustering (K-Means)-  Dim Reduction (PCA)-  Anomaly Detection (Isolation Forest)   | -  고객 세분화 (마케팅)-  이상 징후/사기 탐지-  데이터 전처리 및 시각화                          | -  라벨링 비용 0원.-  단독 사용보다는 **지도 학습 전 단계(라벨링 자동화)**로 많이 씀.                  |
| 3. 준지도 학습(Semi-supervised)   | "하나를 배우고 열을 알기"소량의 정답으로 기준을 잡고, 나머지 무정답 데이터를 스스로 라벨링함. | 소량 정답 + 대량 무정답                | -  Self-training (Pseudo-labeling)-  GAN (생성적 적대 신경망)                                  | -  의료 영상 분석 (전문의 라벨링 비쌈)-  저자원 언어 번역                                   | -  가성비 최강 전략.-  '비지도 분류 -> 지도 학습' 파이프라인이 여기에 속함.                         |
| 4. 자기 지도 학습(Self-supervised) | "빈칸 채우기 놀이"데이터의 일부를 가리고(Mask) 그것을 맞추며 문맥을 배움.          | 무정답 데이터(데이터 자체가 정답)           | -  Masked LM (BERT)-  Causal LM (GPT, Llama)-  Contrastive (SimCLR)                    | -  LLM 사전 학습 (Pre-training)-  문맥/의미 이해 (Representation)-  NLU 기본 모델 생성 | -  LLM의 '두뇌'를 만드는 단계.-  엄청난 컴퓨팅 자원이 필요하며 주로 빅테크 기업이 수행함.                 |
| 5. 강화 학습(Reinforcement)      | "당근과 채찍"시행착오를 겪으며 보상(점수)을 최대화하는 행동을 배움.                | 환경(Env) + 보상(Reward)          | -  Q-Learning / DQN-  RLHF (PPO, DPO)                                                  | -  알파고 (게임/바둑)-  로봇 제어 (보행 학습)-  LLM 윤리/말투 교정                          | -  LLM의 '예절'을 가르치는 단계.-  지도 학습만으로 해결 안 되는 미묘한 뉘앙스(Human Preference)를 잡음. |




### 모델 학습 단계

| 단계 (Stage)                   | 학습 유형 (Type)                | 주체 (Who)          | 데이터 (Input)                     | 목표 (Goal)                           | 상태 (Status)               | 비유 (Analogy)               |
| ---------------------------- | --------------------------- | ----------------- | ------------------------------- | ----------------------------------- | ------------------------- | -------------------------- |
| 1. 사전 학습(Pre-training)       | ④ 자기 지도 학습(Self-supervised) | 제조사(Meta, Google) | Raw Text(인터넷 전체)정답 없음           | "지능의 탄생"언어 문법, 세계 지식, 논리력 습득        | Base Model(말은 잘하는데 멍청함)   | 대학 교육(전공 서적 1만 권 읽고 지식 쌓기) |
| 2. 지시 튜닝(Instruction Tuning) | ① 지도 학습(Supervised / SFT)   | 제조사(Meta, Google) | Q&A Pair(지시+답변)정답 있음            | "대화 능력 장착"질문에 답하고 명령을 수행하는 법 습득     | Instruct Model(일 잘하는 비서)  | 신입 연수(업무 매뉴얼과 시키는 일 배우기)   |
| 3. 인간 피드백(Alignment / RLHF)  | ⑤ 강화 학습(Reinforcement)      | 제조사(Meta, Google) | Preference(A보다 B가 좋아)점수(Reward) | "예절과 윤리 주입"사람이 선호하는 말투와 안전성 확보      | Chat Model(예의 바르고 안전한 비서) | 인성 교육(동료들과 잘 지내는 법 배우기)    |
| 4. 추가 학습(Fine-tuning)        | ① 지도 학습(Supervised)         | 사용자(나, 개발자)       | Custom Data(우리 회사 데이터)정답 있음     | "도메인 전문가 특화"특정 분야(법률, 의료, 사내 규정) 숙달 | Custom Model(우리 회사 전용 AI) | 부서 배치(우리 팀만의 업무 스타일 익히기)   |


- 예시 
    - Pre-training (④ 자기 지도): 인터넷 글을 읽으며 지식을 쌓음. (Base Model)
    - SFT (① 지도): Q&A 데이터를 보며 질문에 답하는 법을 배움. (Instruct Model)
    - RLHF (⑤ 강화): 사람의 피드백(점수)을 받으며 더 자연스러운 말투를 익힘. (Chat Model)

> "GPT는 ④자기 지도 학습으로 태어나서, ①지도 학습으로 말을 배우고, ⑤강화 학습으로 예절을 익혔습니다."
