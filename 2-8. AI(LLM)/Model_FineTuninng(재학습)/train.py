import configparser
import os
import torch
import numpy as np
from datasets import load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding
)
from sklearn.metrics import accuracy_score, f1_score

# ==========================================
# 1. Config ë¡œë“œ
# ==========================================
config = configparser.ConfigParser()
if not os.path.exists('config.ini'):
    raise FileNotFoundError("âŒ 'config.ini' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
config.read('config.ini')

# [Path] ì„¹ì…˜
MODEL_NAME = config['Path']['model_name']
TRAIN_FILE = os.path.abspath(config['Path']['train_file']) # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
VALID_FILE_RAW = config['Path'].get('valid_file', '').strip()
VALID_FILE = os.path.abspath(VALID_FILE_RAW) if VALID_FILE_RAW else ""
OUTPUT_DIR = config['Path']['output_dir']
CHECKPOINT_DIR = config['Path']['checkpoint_dir']

# [Hyperparameters] ì„¹ì…˜
NUM_LABELS = config.getint('Hyperparameters', 'num_labels')
MAX_LEN = config.getint('Hyperparameters', 'max_seq_length')
BATCH_SIZE = config.getint('Hyperparameters', 'batch_size')
LR = config.getfloat('Hyperparameters', 'learning_rate')
EPOCHS = config.getint('Hyperparameters', 'epochs')
SEED = config.getint('Hyperparameters', 'seed')
SPLIT_RATIO = config.getfloat('Hyperparameters', 'split_ratio')

# [Subset] í…ŒìŠ¤íŠ¸ìš© ì˜µì…˜ (config.iniì— ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ False ì‚¬ìš©)
USE_SUBSET = config.getboolean('Hyperparameters', 'use_subset', fallback=False)
SUBSET_SIZE = config.getint('Hyperparameters', 'subset_size', fallback=100)

print(f"â–¶ ëª¨ë¸: {MODEL_NAME}")
print(f"â–¶ í•™ìŠµ íŒŒì¼: {TRAIN_FILE}")

# ==========================================
# 2. ë°ì´í„°ì…‹ ë¡œë“œ ë° ë¶„í• 
# ==========================================
# 1) Validation íŒŒì¼ ìœ ë¬´ì— ë”°ë¥¸ ë¡œë“œ
if VALID_FILE and os.path.exists(VALID_FILE):
    print(f"â–¶ ê²€ì¦ íŒŒì¼: {VALID_FILE}")
    dataset = load_dataset("parquet", data_files={"train": TRAIN_FILE, "validation": VALID_FILE})
    train_dataset = dataset["train"]
    eval_dataset = dataset["validation"]
else:
    print(f"â–¶ ê²€ì¦ íŒŒì¼: ì—†ìŒ (í•™ìŠµ ë°ì´í„°ì—ì„œ {SPLIT_RATIO*100}% ìë™ ë¶„í• )")
    raw_dataset = load_dataset("parquet", data_files={"train": TRAIN_FILE})
    # ì‹œë“œ ê³ ì •í•˜ì—¬ ë¶„í• 
    split_datasets = raw_dataset["train"].train_test_split(test_size=SPLIT_RATIO, seed=SEED)
    train_dataset = split_datasets["train"]
    eval_dataset = split_datasets["test"]

print(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: í•™ìŠµ({len(train_dataset)}) / ê²€ì¦({len(eval_dataset)})")

# 2) ì»¬ëŸ¼ëª… ë³€ê²½ (Label -> label, Text -> Review_Text í™•ì¸ í•„ìš”)
# ì‹¤ì œ ë°ì´í„°ì…‹ì˜ ì»¬ëŸ¼ëª…ì„ í™•ì¸ í›„ í•„ìš”í•˜ë©´ ìˆ˜ì •
print(f"â–¶ ë°ì´í„°ì…‹ ì»¬ëŸ¼ ëª©ë¡: {train_dataset.column_names}")

if "Label" in train_dataset.column_names:
    train_dataset = train_dataset.rename_column("Label", "label")
    eval_dataset = eval_dataset.rename_column("Label", "label")
    print("  -> ì»¬ëŸ¼ëª… ë³€ê²½ ì™„ë£Œ: Label -> label")

# 3) ë°ì´í„°ì…‹ ì¶•ì†Œ (í…ŒìŠ¤íŠ¸ ëª¨ë“œì¼ ë•Œë§Œ ë™ì‘)
if USE_SUBSET:
    print(f"\nâš  [í…ŒìŠ¤íŠ¸ ëª¨ë“œ] ë°ì´í„°ì…‹ ì¶•ì†Œ (í•™ìŠµ: {SUBSET_SIZE}ê°œ ê¸°ì¤€)")
    
    # í•™ìŠµ ë°ì´í„° ì¶•ì†Œ
    if len(train_dataset) > SUBSET_SIZE:
        train_dataset = train_dataset.select(range(SUBSET_SIZE))
    
    # ê²€ì¦ ë°ì´í„°ë„ ë¹„ìœ¨ì— ë§ì¶° ì¶•ì†Œ (í•™ìŠµ ë°ì´í„°ì˜ 20% í¬ê¸°)
    eval_subset_size = max(int(SUBSET_SIZE * 0.2), 10) # ìµœì†Œ 10ê°œëŠ” ë³´ì¥
    if len(eval_dataset) > eval_subset_size:
        eval_dataset = eval_dataset.select(range(eval_subset_size))
        
    print(f"â–¶ ì¶•ì†Œëœ ë°ì´í„° ê°œìˆ˜: í•™ìŠµ({len(train_dataset)}) / ê²€ì¦({len(eval_dataset)})\n")

# ==========================================
# 3. ì „ì²˜ë¦¬ (Tokenizer)
# ==========================================
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

def preprocess_function(examples):
    # ì£¼ì˜: ë°ì´í„°ì…‹ì˜ ì‹¤ì œ í…ìŠ¤íŠ¸ ì»¬ëŸ¼ëª…ì´ 'Review_Text'ì¸ì§€ 'text'ì¸ì§€ í™•ì¸ í•„ìš”!
    # ì—¬ê¸°ì„œëŠ” ì§ˆë¬¸ìë‹˜ì˜ ë°ì´í„°ì— ë§ì¶° 'Review_Text'ë¡œ ì„¤ì •í•¨
    return tokenizer(
        examples["Review_Text"], 
        truncation=True, 
        padding="max_length", 
        max_length=MAX_LEN
    )

print("â³ ë°ì´í„° í† í¬ë‚˜ì´ì§• ì¤‘...")
tokenized_train = train_dataset.map(preprocess_function, batched=True)
tokenized_eval = eval_dataset.map(preprocess_function, batched=True)

# ==========================================
# 4. ëª¨ë¸ ë° í•™ìŠµ ì‹¤í–‰
# ==========================================
model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=NUM_LABELS)

def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    acc = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average="macro")
    return {"accuracy": acc, "f1": f1}

training_args = TrainingArguments(
    output_dir=CHECKPOINT_DIR,
    learning_rate=LR,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    num_train_epochs=EPOCHS,
    weight_decay=0.01,
    eval_strategy="epoch",  # ìµœì‹  ë²„ì „ í˜¸í™˜ (evaluation_strategy -> eval_strategy)
    save_strategy="epoch",
    load_best_model_at_end=True,
    save_total_limit=2,
    seed=SEED,
    dataloader_pin_memory=False, # CPU í™˜ê²½ ë“±ì—ì„œ ê²½ê³  ë°©ì§€
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_eval,
    processing_class=tokenizer, # tokenizer -> processing_class (ìµœì‹  ê²½ê³  ëŒ€ì‘)
    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),
    compute_metrics=compute_metrics,
)

print("ğŸš€ í•™ìŠµ ì‹œì‘...")
trainer.train()

# ==========================================
# 5. ìµœì¢… ì €ì¥
# ==========================================
print(f"ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥ ì¤‘... ({OUTPUT_DIR})")
trainer.save_model(OUTPUT_DIR)
print("ğŸ‰ ì™„ë£Œ!")

