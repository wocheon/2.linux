{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7G6AGJFKiv7"
   },
   "source": [
    "# Model FineTunning ì‹¤í–‰ ë° í‰ê°€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBsDoGXSK18g"
   },
   "source": [
    "## ê°œìš”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTPCLPNDLGO1"
   },
   "source": [
    "* GPUì‚¬ìš© ë¶ˆê°€ í™˜ê²½ì—ì„œ ëª¨ë¸ í•™ìŠµ(Finetunning)ì„ ì§„í–‰í•˜ê¸° ìœ„í•œ colab notbook\n",
    "* Google Driveì— ì´ˆê¸° ëª¨ë¸ ë° ì‚¬ìš©í•  ë°ì´í„°ì…‹ ì—…ë¡œë“œ í•„ìš”\n",
    "* í•™ìŠµ ì™„ë£Œ í›„ ë°ì´í„°ì…‹ì„ í•˜ë‚˜ë¡œ í•©ì³ì„œ í•˜ë‚˜ì˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ë§Œë“¤ê³  ì´ë¥¼ í†µí•´ ëª¨ë¸ í‰ê°€ ìˆ˜í–‰\n",
    "* ëª¨ë¸ í‰ê°€ ì™„ë£Œ í›„ Huggingfaceì— ëª¨ë¸ ì—…ë¡œë“œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Am6WN2BA29Nt"
   },
   "source": [
    "## * ëª¨ë¸ ì‚¬ìš©ì‹œ GPU ì‚¬ìš© í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q_ePEsflIVt6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"ì‚¬ìš© ê°€ëŠ¥í•œ ì¥ì¹˜: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
    "# ì¶œë ¥ ì˜ˆì‹œ: ì‚¬ìš© ê°€ëŠ¥í•œ ì¥ì¹˜: Tesla T4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bLWY0-j3jBT"
   },
   "source": [
    "## * Google Drive ì—°ê²° ë° ë§ˆìš´íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kfpbk_3MI_6K"
   },
   "outputs": [],
   "source": [
    "# 1. êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—°ê²°\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "print(\"ğŸ“‚ êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—°ê²° ì¤‘...\")\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2. ì‘ì—… ë””ë ‰í† ë¦¬ ë³€ê²½ (ì„œë²„ì˜ cd ëª…ë ¹ì™€ ë™ì¼)\n",
    "# ì‹¤ì œ í”„ë¡œì íŠ¸ í´ë” ê²½ë¡œë¡œ ì´ë™\n",
    "PROJECT_PATH = '/content/drive/MyDrive/sentiment_project'\n",
    "\n",
    "if not os.path.exists(PROJECT_PATH):\n",
    "    # í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„± (ìµœì´ˆ 1íšŒ)\n",
    "    os.makedirs(PROJECT_PATH)\n",
    "    print(f\"ğŸ†• í´ë” ìƒì„±ë¨: {PROJECT_PATH}\")\n",
    "    print(\"âš  êµ¬ê¸€ ë“œë¼ì´ë¸Œì— ê°€ì„œ í•„ìš”í•œ íŒŒì¼(train.py, config.ini ë“±)ì„ ë„£ì–´ì£¼ì„¸ìš”!\")\n",
    "else:\n",
    "    os.chdir(PROJECT_PATH)\n",
    "    print(f\"âœ… ì‘ì—… ê²½ë¡œ ì´ë™ ì™„ë£Œ: {os.getcwd()}\")\n",
    "\n",
    "# 3. í˜„ì¬ íŒŒì¼ ëª©ë¡ í™•ì¸ (ì œëŒ€ë¡œ ì´ë™í–ˆëŠ”ì§€ ì²´í¬)\n",
    "print(\"\\n[í˜„ì¬ í´ë” íŒŒì¼ ëª©ë¡]\")\n",
    "print(os.listdir())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUSnvSTrReKE"
   },
   "outputs": [],
   "source": [
    "# ì‘ì—… ë””ë ‰í† ë¦¬ ì´ë™\n",
    "%cd /content/drive/MyDrive/sentiment_project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDPO2OW937oq"
   },
   "source": [
    "## * Model Fine-Tunning ì½”ë“œ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NLcdC4Ig3tfh"
   },
   "outputs": [],
   "source": [
    "# Fine-tunning ì½”ë“œ ì‹¤í–‰ì„ ìœ„í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸\n",
    "!pip install transformers datasets torch scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yh3NKUJISlPF"
   },
   "outputs": [],
   "source": [
    "# Fine-tunning ì½”ë“œ ì‹¤í–‰\n",
    "!python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ftOoxe43DCcJ"
   },
   "source": [
    "## * Model í‰ê°€ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0_NMoa208Lm8"
   },
   "outputs": [],
   "source": [
    "!pip install evaluate wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7NUPcP7H6-Rc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "# ==========================================================\n",
    "# 1. ì„¤ì • (ê²½ë¡œ ë° ì»¬ëŸ¼ëª… í™•ì¸ í•„ìˆ˜)\n",
    "# ==========================================================\n",
    "# WandB ë„ê¸° (ì—ëŸ¬ ë°©ì§€)\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# ëª¨ë¸ ê²½ë¡œ\n",
    "MODEL_PATH = \"./models/fine_tunned_kobert\"\n",
    "\n",
    "# ë°ì´í„° íŒŒì¼ë“¤ì´ ìˆëŠ” í´ë” ê²½ë¡œ (ë§ˆì§€ë§‰ì— ìŠ¬ë˜ì‹œ ì—†ì–´ë„ ë¨)\n",
    "DATA_DIR = \"./dataset_dir\"\n",
    "\n",
    "# CSV ë‚´ ì»¬ëŸ¼ ì´ë¦„ (ì‹¤ì œ íŒŒì¼ê³¼ ì¼ì¹˜í•´ì•¼ í•¨!)\n",
    "TEXT_COLUMN = \"text\"\n",
    "LABEL_COLUMN = \"label\"\n",
    "\n",
    "# ==========================================================\n",
    "# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "# ==========================================================\n",
    "print(f\"â³ ëª¨ë¸ ë¡œë“œ ì¤‘: {MODEL_PATH}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# ==========================================================\n",
    "# 3. ë°ì´í„° ë¡œë“œ (ì—¬ëŸ¬ CSV í•©ì¹˜ê¸°)\n",
    "# ==========================================================\n",
    "# í´ë” ë‚´ ëª¨ë“  .csv íŒŒì¼ ì°¾ê¸°\n",
    "csv_files = glob.glob(os.path.join(DATA_DIR, \"*.csv\"))\n",
    "\n",
    "if not csv_files:\n",
    "    raise FileNotFoundError(f\"âŒ '{DATA_DIR}' ê²½ë¡œì— .csv íŒŒì¼ì´ í•˜ë‚˜ë„ ì—†ìŠµë‹ˆë‹¤!\")\n",
    "\n",
    "print(f\"ğŸ“‚ ë°œê²¬ëœ ë°ì´í„° íŒŒì¼ ({len(csv_files)}ê°œ):\")\n",
    "for f in csv_files:\n",
    "    print(f\" - {f}\")\n",
    "\n",
    "# ì—¬ëŸ¬ íŒŒì¼ì„ í•œ ë²ˆì— ë¡œë“œ (ìë™ìœ¼ë¡œ í•©ì³ì§)\n",
    "# split=\"train\"ì„ ì§€ì •í•´ì•¼ DatasetDictê°€ ì•„ë‹Œ Dataset ê°ì²´ê°€ ë‚˜ì˜´\n",
    "combined_dataset = load_dataset(\"csv\", data_files=csv_files, split=\"train\")\n",
    "\n",
    "print(f\"âœ… ì´ ë°ì´í„° ê°œìˆ˜: {len(combined_dataset)}\")\n",
    "\n",
    "# ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def preprocess_function(examples):\n",
    "    # Noneì´ë‚˜ ë¹„ë¬¸ìì—´ ë°ì´í„° ë°©ì–´ ì½”ë“œ\n",
    "    texts = [str(x) if x is not None else \"\" for x in examples[TEXT_COLUMN]]\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "print(\"âš™ï¸ ë°ì´í„° ì „ì²˜ë¦¬(í† í°í™”) ì¤‘...\")\n",
    "tokenized_dataset = combined_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# ì—¬ê¸°ì„œëŠ” 'ì „ì²´ ë°ì´í„°ë¥¼ í‰ê°€ìš©'ìœ¼ë¡œ ì“´ë‹¤ê³  ê°€ì • (ì´ë¯¸ í•™ìŠµì´ ëë‚œ ëª¨ë¸ì´ë¯€ë¡œ)\n",
    "# ë§Œì•½ ì—¬ê¸°ì„œë„ ì¼ë¶€ë§Œ ë½‘ê³  ì‹¶ë‹¤ë©´ .select() ë“±ì„ ì‚¬ìš©\n",
    "test_dataset = tokenized_dataset\n",
    "\n",
    "print(f\"âœ… ìµœì¢… í‰ê°€ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(test_dataset)}ê°œ\")\n",
    "\n",
    "# ==========================================================\n",
    "# 4. í‰ê°€ ì§€í‘œ ì •ì˜\n",
    "# ==========================================================\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    # average=\"macro\": í´ë˜ìŠ¤ë³„ ì„±ëŠ¥ í‰ê·  (ë¶ˆê· í˜• ë°ì´í„°ì— ì¢‹ìŒ)\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy[\"accuracy\"],\n",
    "        \"f1\": f1[\"f1\"]\n",
    "    }\n",
    "\n",
    "# ==========================================================\n",
    "# 5. í‰ê°€ ì‹¤í–‰\n",
    "# ==========================================================\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./temp_eval_results\",\n",
    "    report_to=\"none\",\n",
    "    per_device_eval_batch_size=64  # ì†ë„ë¥¼ ìœ„í•´ ë°°ì¹˜ í¬ê²Œ\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"ğŸš€ í‰ê°€ ì‹œì‘...\")\n",
    "metrics = trainer.evaluate()\n",
    "\n",
    "print(\"\\nğŸ“Š ìµœì¢… í‰ê°€ ì„±ì í‘œ:\")\n",
    "print(\"=\" * 30)\n",
    "for key, value in metrics.items():\n",
    "    print(f\"{key}: {value:.4f}\")\n",
    "print(\"=\" * 30)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fZaQqiLJDYDS"
   },
   "source": [
    "## * Hugging Faceì— ëª¨ë¸ ì—…ë¡œë“œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6xgfWzqrEm36"
   },
   "source": [
    "* Huggingface Access Token ìƒì„±\n",
    "  * ëª¨ë¸ ì—…ë¡œë“œë¥¼ ìœ„í•´ Write Access Tokenì„ ìƒì„±\n",
    "  * https://huggingface.co/settings/tokens > 'create new token'\n",
    "  * Typeì„ Writeë¡œ ì§€ì •í•˜ì—¬ ìƒì„±\n",
    "\n",
    "* Huggingface Write Tokenì„ í™˜ê²½ë³€ìˆ˜ë¡œ ì§€ì •\n",
    "  * HF_TOKEN\n",
    "    * Huggingface Write tokenì„ í‚¤ ê°’ìœ¼ë¡œ ì§€ì •í•„ìš”\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zTFkOmqc4bRx"
   },
   "outputs": [],
   "source": [
    "# í† í°ì„ í†µí•´ HuggingFace ë¡œê·¸ì¸ í™•ì¸\n",
    "import os\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "# 1. í™˜ê²½ë³€ìˆ˜ì— í† í° ì£¼ì… (ì´ ì½”ë“œê°€ ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ë³´ë‹¤ ìœ„ì— ì˜¤ë©´ ì¢‹ìŒ)\n",
    "os.environ[\"HUGGING_FACE_HUB_TOKEN\"] = userdata.get('HF_TOKEN')\n",
    "\n",
    "# 2. ì´ì œ login() í˜¸ì¶œ ì—†ì´ë„ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥\n",
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "\n",
    "user_info = api.whoami()\n",
    "print(f\"ë¡œê·¸ì¸ëœ ì‚¬ìš©ì: {user_info['name']}\")\n",
    "\n",
    "# íŒŒì¼ì—…ë¡œë“œ\n",
    "api.upload_folder(folder_path=\"models/fine_tunned_koelectra\"\n",
    "              , repo_id=\"wocheon/fine_tunned_koelectra\"\n",
    "              , repo_type=\"model\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOGC1luEoTy5NFGbLN3uW7f",
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
