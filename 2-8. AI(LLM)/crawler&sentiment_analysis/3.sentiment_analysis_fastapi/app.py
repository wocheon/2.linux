from fastapi import FastAPI
from pydantic import BaseModel
from contextlib import asynccontextmanager
import torch
from transformers import AutoTokenizer, BertForSequenceClassification, ElectraForSequenceClassification
from typing import List, Dict
import configparser
import json
import asyncio
from openai import AsyncOpenAI  # [NEW] OpenAI SDK

# --- Config Î°úÎìú ---
config = configparser.ConfigParser()
config.read('config.ini')

# Î°úÏª¨ Î™®Îç∏ ÏÑ§Ï†ï
KOBERT_MODEL_DIR = config.get('models', 'kobert_dir')
KOELECTRA_MODEL_DIR = config.get('models', 'koelectra_dir')

def load_label_map(section):
    return {int(k): v for k, v in config[section].items()}
KOBERT_LABEL_MAP = load_label_map('labels_kobert')
KOELECTRA_LABEL_MAP = load_label_map('labels_koelectra')

# LLM ÏÑ§Ï†ï
LLM_BASE_URL = config.get('llm', 'api_base')
LLM_API_KEY = config.get('llm', 'api_key', fallback="EMPTY")
LLM_MODEL_NAME = config.get('llm', 'model_name')
LLM_TIMEOUT = config.getint('llm', 'timeout', fallback=60)
LLM_LABELS = config.get('llm', 'sentiment_labels')
LLM_PROMPT = config.get('llm', 'prompt_template')

# Ï†ÑÏó≠ Î¶¨ÏÜåÏä§
ml_resources: Dict = {}

# [NEW] OpenAI ÎπÑÎèôÍ∏∞ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ ÏÉùÏÑ±
# vLLMÏù¥ÎÇò Ollama Îì± OpenAI Ìò∏Ìôò ÏÑúÎ≤ÑÎùºÎ©¥ base_urlÎßå ÎßûÏ∂îÎ©¥ Îê®
aclient = AsyncOpenAI(
    base_url=LLM_BASE_URL,
    api_key=LLM_API_KEY,
    timeout=LLM_TIMEOUT
)

@asynccontextmanager
async def lifespan(app: FastAPI):
    # (Î°úÏª¨ Î™®Îç∏ Î°úÎî© Î°úÏßÅÏùÄ Í∏∞Ï°¥Í≥º ÎèôÏùº)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"üöÄ Loading local models on {device}...")
    
    try:
        # KoBERT
        ml_resources["kobert_tokenizer"] = AutoTokenizer.from_pretrained(KOBERT_MODEL_DIR, trust_remote_code=True)
        ml_resources["kobert_model"] = BertForSequenceClassification.from_pretrained(KOBERT_MODEL_DIR).to(device)
        
        # KoELECTRA
        ml_resources["koelectra_tokenizer"] = AutoTokenizer.from_pretrained(KOELECTRA_MODEL_DIR, trust_remote_code=True)
        ml_resources["koelectra_model"] = ElectraForSequenceClassification.from_pretrained(KOELECTRA_MODEL_DIR).to(device)
        
        ml_resources["device"] = device
        print("‚úÖ Local models loaded.")
    except Exception as e:
        print(f"‚ùå Error loading models: {e}")
        raise e

    yield
    ml_resources.clear()
    if torch.cuda.is_available(): torch.cuda.empty_cache()

app = FastAPI(lifespan=lifespan)

class BatchInputText(BaseModel):
    texts: List[str]

# --- Î°úÏª¨ Î™®Îç∏ Ï∂îÎ°† (Í∏∞Ï°¥ Ïú†ÏßÄ) ---
def predict_local_batch(tokenizer, model, label_map, texts, device):
    # ... (Í∏∞Ï°¥ ÏΩîÎìúÏôÄ ÎèôÏùº: tokenizer -> model -> softmax -> argmax)
    inputs = tokenizer(texts, return_tensors="pt", truncation=True, max_length=256, padding=True).to(device)
    with torch.no_grad():
        outputs = model(**inputs)
        probs = torch.softmax(outputs.logits, dim=1)
        confidences, pred_labels = torch.max(probs, dim=1)
    
    results = []
    for i in range(len(texts)):
        idx = pred_labels[i].item()
        results.append({"sentiment": label_map.get(idx, str(idx)), "score": confidences[i].item()})
    return results

# --- [NEW] OpenAI SDK Í∏∞Î∞ò LLM Ìò∏Ï∂ú ---
async def call_llm_single(text: str):
    prompt = LLM_PROMPT.format(text=text[:1000], labels=LLM_LABELS)
    
    try:
        response = await aclient.chat.completions.create(
            model=LLM_MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            temperature=0.0,
            max_tokens=150      # ÏùëÎãµ Î∂àÍ∞ÄÏãú Ï°∞Ï†ï ÌïÑÏöî
        )
        content = response.choices[0].message.content
        
        # JSON ÌååÏã±
        clean = content.replace("``````", "").strip()
        parsed = json.loads(clean)
        if 'sentiment' in parsed:
            parsed['sentiment'] = parsed['sentiment'].lower()
        return parsed

    except Exception as e:
        print(f"LLM Error: {e}")
        return {"sentiment": "error", "score": 0.0, "msg": str(e)}

# --- ÎùºÏö∞ÌÑ∞ ---
@app.post("/predict/kobert/batch")
def kobert_batch(req: BatchInputText):
    res = predict_local_batch(ml_resources["kobert_tokenizer"], ml_resources["kobert_model"], KOBERT_LABEL_MAP, req.texts, ml_resources["device"])
    return {"model": "kobert", "count": len(res), "results": res}

@app.post("/predict/koelectra/batch")
def koelectra_batch(req: BatchInputText):
    res = predict_local_batch(ml_resources["koelectra_tokenizer"], ml_resources["koelectra_model"], KOELECTRA_LABEL_MAP, req.texts, ml_resources["device"])
    return {"model": "koelectra", "count": len(res), "results": res}

@app.post("/predict/llm/batch")
async def llm_batch(req: BatchInputText):
    # asyncio.gatherÎ°ú Î≥ëÎ†¨ Ìò∏Ï∂ú
    tasks = [call_llm_single(text) for text in req.texts]
    results = await asyncio.gather(*tasks)
    return {"model": LLM_MODEL_NAME, "count": len(results), "results": results}

